{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Nama : Oktaviana Dwi Sujatmiko NIM : 180411100021 TEKNIK INFORMATIKA PENAMBANGAN DATA 3B Statistik Deskriptif Statistik deskriptif adalah salah satu bagian dari ilmu statistika yang berhubungan dengan aktivitas penghimpunan, penataan, peringkasan dan penyajian data dengan harapan agar data lebih bermakna, mudah dibaca dan mudah dipahami oleh pengguna data. Statistik deskriptif hanya sebatas memberikan deskripsi atau gambaran umum tentang karakteristik objek yang diteliti tanpa maksud untuk melakukan generalisasi sampel terhadap populasi. \u200b Kegiatan dalam statistik deskriptif meliputi pengumpulan, pengelompokan dan pengolahan data yang selanjutnya akan menghasilkan ukuran-ukuran statistik seperti frekuensi, pemusatan data, penyebaran data, kecenderungan suatu gugus data dan lain-lain. Selain itu, agar data lebih mudah dibaca dan dipahami maka data dapat diringkas dalam bentuk tabulasi atau disajikan dalam bentuk grafik atau diagram, beberapa yang akan kita bahas yaitu : Rata-rata Median Modus Variansi Standar Deviasi Range Quartil Skewness ## 1. Rata - rata \u200b Rata-rata adalah nilai yang mewakili sekelompok data. Rata-rata diperoleh dari penjumlahan seluruh nilai data dibagi dengan banyaknya data. ## 2. Median \u200b Untuk data miring (asimetris), ukuran pusat data yang lebih baik adalah median, yang merupakan nilai tengah dalam satu set nilai data yang diurutkan. Ini adalah nilai yang memisahkan separuh data yang lebih tinggi dari data tersebut dan sebagian data yang lebih rendah dari data tersebut. Dalam probabilitas dan statistik, median umumnya berlaku untuk data numerik; namun, kami dapat memperluas konsep menjadi data ordinal. Misalkan kumpulan N data yang diberikan untuk atribut X diurutkan dalam urutan naik. Jika N ganjil, maka median adalah nilai tengah dari data yang ordinal. Jika N adalah genap, maka mediannya tidak unik; dihitung dengan rata rata dari nilai 3. Modus Modus adalah nilai yang sering muncul dalam sekelompok data. 4. Variansi dan standart Variansi Varian adalah rata-rata jarak kuadrat antara data ke rata-rata. 5. Range Jangkauan (Range) adalah ukuran dari selisih nilai maksimum dengan nilai minimum yang ada dalam sekelompok data.R = x_{max} - x_{min} R =xmax\u2212xmin 6. Quartil Kuartil memberikan gambaran pusat distribus, penyebaran, dan bentuk distribusi. Kuartil satu, dilambangkan oleh Q1, adalah persentil ke-25. Nilai ini menunjukan 25% terendah dari data. Kuartil ketiga, dilambangkan oleh Q3, adalah persentil ke-75 - itu memisahkan data 75% dari terendah data (atau 25% dari tertinggi data. 7. Skewness \u200b Derajat distorsi dari kurva lonceng simetris atau distribusi normal. Ini mengukur kurangnya simetri dalam distribusi data Untuk menghitung derajat distorisi dapat menggunakan Koefisien Kemencengan Pearson yang diperoleh dengan menggunakan nilai selisih rata-rata dengan modus dibagi simpangan baku. Mengukur Jarak Atribut Binary Mengukur Jarak Tipe categorical Overlay Metric Ketika semua atribut adalah bertipe nominal, ukuran jarak yang paling sederhana adalah dengan Ovelay Metric (OM) yang dinyatakan dengan dimana nn adalah banyaknya atribut, ai(x)ai(x) dan ai(y)ai(y) adalah nilai atribut ke ii yaitu AiAi dari masing masing objek xx dan yy, \u03b4 (ai(x),ai(y))\u03b4 (ai(x),ai(y)) adalah 0 jika ai(x)=ai(y)ai(x)=ai(y) dan 1 jika sebaliknya. OM banyak digunakan oleh instance-based learning dan locally weighted learning. Jelas sekali , ini sedikit beruk untuk mengukur jarak antara masing-masing pasangan sample, karena gagal memanfaatkan tambahan informasi yang diberikan oleh nilai atribut nominal yang bisa membantu dalam generalisasi. Value Difference Metric (VDM) VDM dikenalkan oleh Standfill and Waltz, versi sederhana dari VDM tanpa skema pembobotan didefinsisikan dengan dimana CCadalah banyaknya kelas, P(c|ai(x))P(c|ai(x)) adalah probabilitas bersyarat dimana kelas xx adalah cc dari atribut AiAi, yang memiliki nilai ai(x)ai(x), P(c|ai(y))P(c|ai(y)) adalah probabilitas bersyarat dimana kelas yy adalah cc dengan atribut AiAi memiliki nilai ai(y)ai(y) VDM mengasumsikan bahwa dua nilai dari atribut adalah lebih dekat jika memiliki klasifikasi sama. Pendekatan lain berbasi probabilitas adalah SFM (Short and Fukunaga Metric) yang kemudian dikembangkan oleh Myles dan Hand dan didefinisikan. Minimum Risk Metric (MRM) Ukuran ini dipresentasikan oleh Blanzieri and Ricci, berbeda dari SFM yaitu meminimumkan selisih antara kesalahan berhingga dan kesalahan asymtotic. MRM meminimumkan risk of misclassification yang didefinisikan. Mengukur Jarak Tipe Ordinal Nilai-nilai atribut ordinal memiliki urutan atau peringkat, namun besarnya antara nilai-nilai berturut-turut tidak diketahui. Contohnya tingkatan kecil, sedang, besar untuk atribut ukuran. Atribut ordinal juga dapat diperoleh dari diskritisasi atribut numerik dengan membuat rentang nilai ke dalam sejumlah kategori tertentu. Kategori-kategori ini disusun dalam peringkat. Yaitu, rentang atribut numerik dapat dipetakan ke atribut ordinal ff yang memiliki MfMf state. Misalnya, kisaran suhu atribut skala-skala (dalam Celcius)dapat diatur ke dalam status berikut: \u221230 hingga \u221210, \u221210 hingga 10, 10 hingga 30, masing-masing mewakili kategori suhu dingin, suhu sedang, dan suhu hangat. MM adalah jumlah keadaan yang dapat dilakukan oleh atribut ordinalmemiliki. State ini menentukan peringkat 1,...,Mf1,...,Mf Perlakuan untuk atribut ordinal adalah cukup sama dengan atribut numerik ketika menghitung disimilarity antara objek. Misalkan ff adalah atribut-atribut dari atribut ordinal dari nn objek. Menghitung disimilarity terhadap f fitur sebagai berikut: Nilai ff untuk objek ke-ii adalah xifxif, dan ff memiliki MfMf status urutan , mewakili peringkat 1,..,Mf1,..,Mf Ganti setiap xifxif dengan peringkatnya, rif\u2208{1...Mf}rif\u2208{1...Mf} Karena setiap atribut ordinal dapat memiliki jumlah state yang berbeda, diperlukan untuk memetakan rentang setiap atribut ke [0,0, 1.0] sehingga setiap atribut memiliki bobot yang sama. Perl melakukan normalisasi data dengan mengganti peringkat rifrif denganzif=rif\u22121Mf\u22121zif=rif\u22121Mf\u22121 Dissimilarity kemudian dihitung dengan menggunakan ukuran jarak seperti atribut numerik dengan data yang baru setelah ditransformasi $ z _ { i f } ## Menghitung Jarak Tipe Campuran Menghitung ketidaksamaan antara objek dengan atribut campuran yang berupa nominal, biner simetris, biner asimetris, numerik, atau ordinal yang ada pada kebanyakan databasae dapat dinyatakan dengan memproses semua tipe atribut secara bersamaan. Salah satu teknik tersebut menggabungkan atribut yang berbeda ke dalam matriks ketidaksamaan tunggal dan menyatakannya dengan skala interval antar [0,0,1.0][0,0,1.0]. Misalkan data berisi atribut pp tipe campuran. Ketidaksamaan (disimilarity ) antara objek ii dan jj dinyatakan dengan d(i,j)=\u2211pf=1\u03b4(f)ijd(f)ij\u2211pf=1\u03b4(f)ijd(i,j)=\u2211f=1p\u03b4ij(f)dij(f)\u2211f=1p\u03b4ij(f) dimana \u03b4fij=0\u03b4ijf=0 - jika xifxif atau xjfxjf adalah hilang (i.e., tidak ada pengukuran dari atribut f untuk objek ii atau objek jj) jika xif=xjf=0xif=xjf=0 dan atribut ff adalah binary asymmetric, selain itu \u03b4fij=1\u03b4ijf=1 Kontribusi dari atribut ff untuk dissimilarity antara i dan j (yaitu.dfijdijf) dihitung bergantung pada tipenya, Jika ff adalah numerik, dfij=\u2225xif\u2212xjf\u2225maxhxhf\u2212minhxhfdijf=\u2016xif\u2212xjf\u2016maxhxhf\u2212minhxhf, di mana h menjalankan semua nilai objek yang tidak hilang untuk atribut f Jika ff adalah nominal atau binary,$d_{ij}^{f}=0 $jika xif=xjfxif=xjf, sebaliknya dfij=1dijf=1 Jika ff adalah ordinal maka hitung rangking rifrif dan zif=rif\u22121Mf\u22121zif=rif\u22121Mf\u22121 , dan perlakukan zifzif sebagai numerik. SELECTION FEATURE Pemilihan (seleksi) data dari sekumpulan data operasional perlu dilakukan sebelum tahap penggalian informasi dalam KDD dimulai. Data hasil seleksi yang digunakan untuk proses data mining, disimpan dalam suatu berkas, terpisah dari basis data operasional. Entropy Target Entropy (S) merupakan jumlah bit yang diperkirakan dibutuhkan untuk dapat mengekstrak suatu kelas (+ atau -) dari sejumlah data acak pada ruang sampel S. Entropy dapat dikatakan sebagai kebutuhan bit untuk menyatakan suatu kelas. Entropy digunakan untuk mengukur ketidakaslian S. Information GAIN Gain (S,A) merupakan perolehan informasi dari atribut A relative terhadap output data S. Perolehan informasi didapat dari output data atau variable dependent S yang dikelompokkan berdasarkan atribut A, dinotasikan dengan gain (S,A). Naive Bayes Algoritma Naive Bayes merupakan sebuah metoda klasifikasi menggunakan metode probabilitas dan statistik yg dikemukakan oleh ilmuwan Inggris Thomas Bayes. Algoritma Naive Bayes memprediksi peluang di masa depan berdasarkan pengalaman di masa sebelumnya sehingga dikenal sebagai Teorema Bayes. Ciri utama dr Na\u00efve Bayes Classifier ini adalah asumsi yg sangat kuat (na\u00eff) akan independensi dari masing-masing kondisi / kejadian. Naive Bayes Classifier bekerja sangat baik dibanding dengan model classifier lainnya. Hal ini dibuktikan pada jurnal Xhemali, Daniela, Chris J. Hinde, and Roger G. Stone. \u201cNaive Bayes vs. decision trees vs. neural networks in the classification of training web pages.\u201d (2009), mengatakan bahwa \u201cNa\u00efve Bayes Classifier memiliki tingkat akurasi yg lebih baik dibanding model classifier lainnya\u201d. Keuntungan penggunan adalah bahwa metoda ini hanya membutuhkan jumlah data pelatihan (training data) yang kecil untuk menentukan estimasi parameter yg diperlukan dalam proses pengklasifikasian. Karena yg diasumsikan sebagai variabel independent, maka hanya varians dari suatu variabel dalam sebuah kelas yang dibutuhkan untuk menentukan klasifikasi, bukan keseluruhan dari matriks kovarians. Tahapan dari proses algoritma Naive Bayes adalah: Menghitung jumlah kelas / label. Menghitung Jumlah Kasus Per Kelas Kalikan Semua Variable Kelas Bandingkan Hasil Per Kelas 1. Kelebihan & Kekurangan Naive Bayes Kelebihan Mudah untuk dibuat Hasil bagus Kekurangan Asumsi independence antar atribut membuat akurasi berkurang (karena biasanya ada keterkaitan) Keterangan : x : Data dengan class yang belum diketahui c : Hipotesis data merupakan suatu class spesifik P(c|x) : Probabilitas hipotesis berdasar kondisi (posteriori probability) P(c) : Probabilitas hipotesis (prior probability) P(x|c) : Probabilitas berdasarkan kondisi pada hipotesis P(x) : Probabilitas c Rumus diatas menjelaskan bahwa peluang masuknya sampel karakteristik tertentu dalam kelas C (Posterior) adalah peluang munculnya kelas C (sebelum masuknya sampel tersebut, seringkali disebut prior), dikali dengan peluang kemunculan karakteristik karakteristik sampel pada kelas C (disebut juga likelihood), dibagi dengan peluang kemunculan karakteristik sampel secara global ( disebut juga evidence). Nilai Evidence selalu tetap untuk setiap kelas pada satu sampel. Nilai dari posterior tersebut nantinya akan dibandingkan dengan nilai nilai posterior kelas lainnya untuk menentukan ke kelas apa suatu sampel akan diklasifikasikan. Penjabaran lebih lanjut rumus Bayes tersebut dilakukan dengan menjabarkan (c|x1,\u2026,xn) menggunakan aturan perkalian Dapat dilihat bahwa hasil penjabaran tersebut menyebabkan semakin banyak dan semakin kompleksnya faktor faktor syarat yang mempengaruhi nilai probabilitas, yang hampir mustahil untuk dianalisa satu persatu. Akibatnya, perhitungan tersebut menjadi sulit untuk dilakukan. Disinilah digunakan asumsi independensi yang sangat tinggi (naif), bahwa masing masing petunjuk saling bebas (independen) satu sama lain. Dengan asumsi tersebut, maka berlaku suatu kesamaan sebagai berikut: Persamaan diatas merupakan model dari Teorema Naive Bayes yang selanjutnya akan digunakan dalam proses klasifikasi. Untuk klasifikasi dengan data kontinyu digunakan rumus Densitas Gauss : pengambilan data set from sklearn import datasets` `from pandas import *` `from numpy import *` `from math import *` `from IPython.display import HTML, display; from tabulate import tabulate` def table(df): display(HTML(tabulate(df, tablefmt='html', headers='keys', showindex=False)))` `from sklearn import datasets` `from pandas import *` `from numpy import *` `from math import *` `from IPython.display import HTML, display; from tabulate import tabulate` `def table(df): display(HTML(tabulate(df, tablefmt='html', headers='keys', showindex=False))) sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 4.4 3 1.3 0.2 setosa 5 3.6 1.4 0.2 setosa 7.7 3.8 6.7 2.2 virginica 7.2 3.2 6 1.8 virginica 5 3.4 1.6 0.4 setosa 6.4 2.7 5.3 1.9 virginica 5 3.5 1.3 0.3 setosa 6.2 2.2 4.5 1.5 versicolor 6.3 2.8 5.1 1.5 virginica 5.7 4.4 1.5 0.4 setosa 4.6 3.6 1 0.2 setosa 7.4 2.8 6.1 1.9 virginica 5.5 2.4 3.8 1.1 versicolor 6 3.4 4.5 1.6 versicolor 4.9 3.1 1.5 0.2 setosa 4.5 2.3 1.3 0.3 setosa 7.3 2.9 6.3 1.8 virginica 6.5 3 5.5 1.8 virginica 5.5 2.4 3.7 1 versicolor 6.9 3.2 5.7 2.3 virginica 7.2 3 5.8 1.6 virginica 4.9 3 1.4 0.2 setosa 6.8 3.2 5.9 2.3 virginica 5.6 3 4.5 1.5 versicolor 5 2.3 3.3 1 versicolor 5.6 2.5 3.9 1.1 versicolor 6.3 2.9 5.6 1.8 virginica 6.4 3.2 5.3 2.3 virginica 4.9 2.4 3.3 1 versicolor 4.9 3.6 1.4 0.1 setosa test = [5,6,8,9] print(\"sampel data: \", test) sampel data: [5, 6, 8, 9] identifikasi per grub class target untuk data mining serta perhitungan Probabilitas Prior dan Likehood dataset_classes = {}` `for key,group in dataset.groupby('class'):` `mu_s = [group[c].mean() for c in group.columns[:-1]]` `sigma_s = [group[c].std() for c in group.columns[:-1]]` `dataset_classes[key] = [group, mu_s, sigma_s]` `print(key, \"===>\")` `print('Mu_s =>', array(mu_s))` `print('Sigma_s =>', array(sigma_s))` `table(group)` `def numericalPriorProbability(v, mu, sigma):` `return (1.0/sqrt(2 * pi * (sigma ** 2))*exp(-((v-mu)**2)/(2*(sigma**2))))` `def categoricalProbability(sample,universe):` `return sample.shape[0]/universe.shape[0]` `Ps = ([[y]+[numericalPriorProbability(x, d[1][i], d[2][i]) for i,x in enumerate(test)]+` `[categoricalProbability(d[0],dataset)] for y,d in dataset_classes.items()])` `table(DataFrame(Ps, columns=[\"classes\"]+[\"P( %d | C )\" % d for d in test]+[\"P( C )\"]))` `Pss = ([[r[0], prod(r[1:])] for r in Ps])` `PDss = DataFrame(Pss, columns=['class', 'probability']).sort_values('probability')[::-1]` `table(PDss) setosa ===> Mu_s => [4.89 3.35 1.37 0.25] Sigma_s => [0.36040101 0.55025247 0.16363917 0.09718253] sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 4.4 3 1.3 0.2 setosa 5 3.6 1.4 0.2 setosa 5 3.4 1.6 0.4 setosa 5 3.5 1.3 0.3 setosa 5.7 4.4 1.5 0.4 setosa 4.6 3.6 1 0.2 setosa 4.9 3.1 1.5 0.2 setosa 4.5 2.3 1.3 0.3 setosa 4.9 3 1.4 0.2 setosa 4.9 3.6 1.4 0.1 setosa versicolor ===> Mu_s => [5.5375 2.575 3.9375 1.225 ] Sigma_s => [0.44057592 0.40970373 0.51252178 0.26049404] sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 6.2 2.2 4.5 1.5 versicolor 5.5 2.4 3.8 1.1 versicolor 6 3.4 4.5 1.6 versicolor 5.5 2.4 3.7 1 versicolor 5.6 3 4.5 1.5 versicolor 5 2.3 3.3 1 versicolor 5.6 2.5 3.9 1.1 versicolor 4.9 2.4 3.3 1 versicolor virginica ===> Mu_s => [6.86666667 3.05833333 5.775 1.93333333] Sigma_s => [0.48679533 0.29374799 0.45949577 0.27743413] sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 7.7 3.8 6.7 2.2 virginica 7.2 3.2 6 1.8 virginica 6.4 2.7 5.3 1.9 virginica 6.3 2.8 5.1 1.5 virginica 7.4 2.8 6.1 1.9 virginica 7.3 2.9 6.3 1.8 virginica 6.5 3 5.5 1.8 virginica 6.9 3.2 5.7 2.3 virginica 7.2 3 5.8 1.6 virginica 6.8 3.2 5.9 2.3 virginica 6.3 2.9 5.6 1.8 virginica 6.4 3.2 5.3 2.3 virginica classes P( 5 | C ) P( 6 | C ) P( 8 | C ) P( 9 | C ) P( C ) setosa 1.05656 6.66694e-06 0 0 0.333333 versicolor 0.43022 6.50426e-16 1.77008e-14 5.48538e-194 0.266667 virginica 0.000525523 2.27126e-22 7.03175e-06 1.87532e-141 0.4 class probability virginica 6.29591e-172 versicolor 7.2453e-224 setosa 0 Percobaan `def predict(sampel):` `priorLikehoods = ([[y]+[numericalPriorProbability(x, d[1][i], d[2][i]) for i,x in enumerate(sampel)]+` `[categoricalProbability(d[0],dataset)] for y,d in dataset_classes.items()])` `products = ([[r[0], prod(r[1:])] for r in priorLikehoods])` `result = DataFrame(products, columns=['class', 'probability']).sort_values('probability')[::-1]` `return result.values[0,0]` `dataset_test = DataFrame([list(d)+[predict(d[:4])] for d in data], columns=list(dataset.columns)+['predicted class (by predict())'])` `table(dataset_test)` sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class predicted class (by predict()) 5.1 3.5 1.4 0.2 setosa setosa 4.9 3 1.4 0.2 setosa setosa 4.7 3.2 1.3 0.2 setosa setosa 4.6 3.1 1.5 0.2 setosa setosa 5 3.6 1.4 0.2 setosa setosa 5.4 3.9 1.7 0.4 setosa setosa 4.6 3.4 1.4 0.3 setosa setosa 5 3.4 1.5 0.2 setosa setosa 4.4 2.9 1.4 0.2 setosa setosa 4.9 3.1 1.5 0.1 setosa setosa 5.4 3.7 1.5 0.2 setosa setosa 4.8 3.4 1.6 0.2 setosa setosa 4.8 3 1.4 0.1 setosa setosa 4.3 3 1.1 0.1 setosa setosa 5.8 4 1.2 0.2 setosa setosa 5.7 4.4 1.5 0.4 setosa setosa 5.4 3.9 1.3 0.4 setosa setosa 5.1 3.5 1.4 0.3 setosa setosa 5.7 3.8 1.7 0.3 setosa setosa 5.1 3.8 1.5 0.3 setosa setosa 5.4 3.4 1.7 0.2 setosa setosa 5.1 3.7 1.5 0.4 setosa setosa 4.6 3.6 1 0.2 setosa setosa 5.1 3.3 1.7 0.5 setosa setosa 4.8 3.4 1.9 0.2 setosa setosa 5 3 1.6 0.2 setosa setosa 5 3.4 1.6 0.4 setosa setosa 5.2 3.5 1.5 0.2 setosa setosa 5.2 3.4 1.4 0.2 setosa setosa 4.7 3.2 1.6 0.2 setosa setosa 4.8 3.1 1.6 0.2 setosa setosa 5.4 3.4 1.5 0.4 setosa setosa 5.2 4.1 1.5 0.1 setosa setosa 5.5 4.2 1.4 0.2 setosa setosa 4.9 3.1 1.5 0.2 setosa setosa 5 3.2 1.2 0.2 setosa setosa 5.5 3.5 1.3 0.2 setosa setosa 4.9 3.6 1.4 0.1 setosa setosa 4.4 3 1.3 0.2 setosa setosa 5.1 3.4 1.5 0.2 setosa setosa 5 3.5 1.3 0.3 setosa setosa 4.5 2.3 1.3 0.3 setosa setosa 4.4 3.2 1.3 0.2 setosa setosa 5 3.5 1.6 0.6 setosa setosa 5.1 3.8 1.9 0.4 setosa setosa 4.8 3 1.4 0.3 setosa setosa 5.1 3.8 1.6 0.2 setosa setosa 4.6 3.2 1.4 0.2 setosa setosa 5.3 3.7 1.5 0.2 setosa setosa 5 3.3 1.4 0.2 setosa setosa 7 3.2 4.7 1.4 versicolor virginica 6.4 3.2 4.5 1.5 versicolor versicolor 6.9 3.1 4.9 1.5 versicolor virginica 5.5 2.3 4 1.3 versicolor versicolor 6.5 2.8 4.6 1.5 versicolor versicolor 5.7 2.8 4.5 1.3 versicolor versicolor 6.3 3.3 4.7 1.6 versicolor virginica 4.9 2.4 3.3 1 versicolor versicolor 6.6 2.9 4.6 1.3 versicolor versicolor 5.2 2.7 3.9 1.4 versicolor versicolor 5 2 3.5 1 versicolor versicolor 5.9 3 4.2 1.5 versicolor versicolor 6 2.2 4 1 versicolor versicolor 6.1 2.9 4.7 1.4 versicolor versicolor 5.6 2.9 3.6 1.3 versicolor versicolor 6.7 3.1 4.4 1.4 versicolor versicolor 5.6 3 4.5 1.5 versicolor versicolor 5.8 2.7 4.1 1 versicolor versicolor 6.2 2.2 4.5 1.5 versicolor versicolor 5.6 2.5 3.9 1.1 versicolor versicolor 5.9 3.2 4.8 1.8 versicolor virginica 6.1 2.8 4 1.3 versicolor versicolor 6.3 2.5 4.9 1.5 versicolor versicolor 6.1 2.8 4.7 1.2 versicolor versicolor 6.4 2.9 4.3 1.3 versicolor versicolor 6.6 3 4.4 1.4 versicolor versicolor 6.8 2.8 4.8 1.4 versicolor virginica 6.7 3 5 1.7 versicolor virginica 6 2.9 4.5 1.5 versicolor versicolor 5.7 2.6 3.5 1 versicolor versicolor 5.5 2.4 3.8 1.1 versicolor versicolor 5.5 2.4 3.7 1 versicolor versicolor 5.8 2.7 3.9 1.2 versicolor versicolor 6 2.7 5.1 1.6 versicolor virginica 5.4 3 4.5 1.5 versicolor versicolor 6 3.4 4.5 1.6 versicolor versicolor 6.7 3.1 4.7 1.5 versicolor virginica 6.3 2.3 4.4 1.3 versicolor versicolor 5.6 3 4.1 1.3 versicolor versicolor 5.5 2.5 4 1.3 versicolor versicolor 5.5 2.6 4.4 1.2 versicolor versicolor 6.1 3 4.6 1.4 versicolor versicolor 5.8 2.6 4 1.2 versicolor versicolor 5 2.3 3.3 1 versicolor versicolor 5.6 2.7 4.2 1.3 versicolor versicolor 5.7 3 4.2 1.2 versicolor versicolor 5.7 2.9 4.2 1.3 versicolor versicolor 6.2 2.9 4.3 1.3 versicolor versicolor 5.1 2.5 3 1.1 versicolor versicolor 5.7 2.8 4.1 1.3 versicolor versicolor 6.3 3.3 6 2.5 virginica virginica 5.8 2.7 5.1 1.9 virginica virginica 7.1 3 5.9 2.1 virginica virginica 6.3 2.9 5.6 1.8 virginica virginica 6.5 3 5.8 2.2 virginica virginica 7.6 3 6.6 2.1 virginica virginica 4.9 2.5 4.5 1.7 virginica versicolor 7.3 2.9 6.3 1.8 virginica virginica 6.7 2.5 5.8 1.8 virginica virginica 7.2 3.6 6.1 2.5 virginica virginica 6.5 3.2 5.1 2 virginica virginica 6.4 2.7 5.3 1.9 virginica virginica 6.8 3 5.5 2.1 virginica virginica 5.7 2.5 5 2 virginica virginica 5.8 2.8 5.1 2.4 virginica virginica 6.4 3.2 5.3 2.3 virginica virginica 6.5 3 5.5 1.8 virginica virginica 7.7 3.8 6.7 2.2 virginica virginica 7.7 2.6 6.9 2.3 virginica virginica 6 2.2 5 1.5 virginica versicolor 6.9 3.2 5.7 2.3 virginica virginica 5.6 2.8 4.9 2 virginica virginica 7.7 2.8 6.7 2 virginica virginica 6.3 2.7 4.9 1.8 virginica virginica 6.7 3.3 5.7 2.1 virginica virginica 7.2 3.2 6 1.8 virginica virginica 6.2 2.8 4.8 1.8 virginica virginica 6.1 3 4.9 1.8 virginica virginica 6.4 2.8 5.6 2.1 virginica virginica 7.2 3 5.8 1.6 virginica virginica 7.4 2.8 6.1 1.9 virginica virginica 7.9 3.8 6.4 2 virginica virginica 6.4 2.8 5.6 2.2 virginica virginica 6.3 2.8 5.1 1.5 virginica virginica 6.1 2.6 5.6 1.4 virginica virginica 7.7 3 6.1 2.3 virginica virginica 6.3 3.4 5.6 2.4 virginica virginica 6.4 3.1 5.5 1.8 virginica virginica 6 3 4.8 1.8 virginica virginica 6.9 3.1 5.4 2.1 virginica virginica 6.7 3.1 5.6 2.4 virginica virginica 6.9 3.1 5.1 2.3 virginica virginica 5.8 2.7 5.1 1.9 virginica virginica 6.8 3.2 5.9 2.3 virginica virginica 6.7 3.3 5.7 2.5 virginica virginica 6.7 3 5.2 2.3 virginica virginica 6.3 2.5 5 1.9 virginica virginica 6.5 3 5.2 2 virginica virginica 6.2 3.4 5.4 2.3 virginica virginica 5.9 3 5.1 1.8 virginica virginica corrects = dataset_test.loc[dataset_test['class'] == dataset_test['predicted class (by predict())']].shape[0] print('Prediksi Training Bayes: %d of %d == %f %%' % (corrects, len(data), corrects / len(data) * 100)) Prediksi Training Bayes: 140 of 150 == 93.333333 % KNN Pengertian KNN K-nearest neighbors atau knn adalah algoritma yang berfungsi untuk melakukan klasifikasi suatu data berdasarkan data pembelajaran ( train data sets ), yang diambil dari k tetangga terdekatnya ( nearest neighbors ). Dengan k merupakan banyaknya tetangga terdekat. A. Cara Kerja Algoritma K-Nearest Neighbors (KNN) K-nearest neighbors melakukan klasifikasi dengan proyeksi data pembelajaran pada ruang berdimensi banyak. Ruang ini dibagi menjadi bagian-bagian yang merepresentasikan kriteria data pembelajaran. Setiap data pembelajaran direpresentasikan menjadi titik-titik c pada ruang dimensi banyak. sepal length sepal width petal length petal width variety sample data 4,9 2,4 3,3 1 Versicolor Training Data a b c d class 4,9 3,1 1,5 0,1 Setosa 4,9 3 1,4 0,2 Setosa 4,8 3,1 1,6 0,2 Setosa 5 3 1,6 0,2 Setosa 4,8 3 1,4 0,1 Setosa 4,9 3,1 1,5 0,2 Setosa 4,8 3 1,4 0,3 Setosa 5 3,3 1,4 0,2 Setosa 4,7 3,2 1,6 0,2 Setosa 4,7 3,2 1,3 0,2 Setosa 4,6 3,1 1,5 0,2 Setosa 5 3,4 1,5 0,2 Setosa 5 3,2 1,2 0,2 Setosa 4,8 3,4 1,6 0,2 Setosa 4,6 3,2 1,4 0,2 Setosa 5,1 3,4 1,5 0,2 Setosa 5 3,4 1,6 0,4 Setosa 5,2 3,4 1,4 0,2 Setosa 4,6 3,4 1,4 0,3 Setosa 5,1 3,3 1,7 0,5 Setosa 5,1 3,5 1,4 0,2 Setosa 5,1 3,5 1,4 0,3 Setosa 5 3,5 1,3 0,3 Setosa 5,2 3,5 1,5 0,2 Setosa 4,8 3,4 1,9 0,2 Setosa 5 3,6 1,4 0,2 Setosa 4,9 3,6 1,4 0,1 Setosa 4,4 3 1,3 0,2 Setosa 4,4 3,2 1,3 0,2 Setosa 4,4 2,9 1,4 0,2 Setosa 5 3,5 1,6 0,6 Setosa 5,4 3,4 1,7 0,2 Setosa 5,4 3,4 1,5 0,4 Setosa 5,1 3,7 1,5 0,4 Setosa 5,3 3,7 1,5 0,2 Setosa 5,1 3,8 1,5 0,3 Setosa 5,1 3,8 1,6 0,2 Setosa 4,3 3 1,1 0,1 Setosa 5,5 3,5 1,3 0,2 Setosa 4,6 3,6 1 0,2 Setosa 5,4 3,7 1,5 0,2 Setosa 5,1 3,8 1,9 0,4 Setosa 4,5 2,3 1,3 0,3 Setosa 5,4 3,9 1,7 0,4 Setosa 5,4 3,9 1,3 0,4 Setosa 5,2 4,1 1,5 0,1 Setosa 5,7 3,8 1,7 0,3 Setosa 5,5 4,2 1,4 0,2 Setosa 5,8 4 1,2 0,2 Setosa 5,7 4,4 1,5 0,4 Setosa 5,1 2,5 3 1,1 Versicolor 5 2,3 3,3 1 Versicolor 5,7 2,6 3,5 1 Versicolor 5 2 3,5 1 Versicolor 5,6 2,9 3,6 1,3 Versicolor 5,5 2,4 3,7 1 Versicolor 5,5 2,4 3,8 1,1 Versicolor 5,6 2,5 3,9 1,1 Versicolor 5,2 2,7 3,9 1,4 Versicolor 5,8 2,7 3,9 1,2 Versicolor 5,5 2,5 4 1,3 Versicolor 5,8 2,6 4 1,2 Versicolor 5,8 2,7 4,1 1 Versicolor 5,5 2,3 4 1,3 Versicolor 5,6 3 4,1 1,3 Versicolor 5,7 2,8 4,1 1,3 Versicolor 6 2,2 4 1 Versicolor 5,7 3 4,2 1,2 Versicolor 6,1 2,8 4 1,3 Versicolor 5,6 2,7 4,2 1,3 Versicolor 5,7 2,9 4,2 1,3 Versicolor 5,9 3 4,2 1,5 Versicolor 5,5 2,6 4,4 1,2 Versicolor 6,2 2,9 4,3 1,3 Versicolor 5,7 2,8 4,5 1,3 Versicolor 5,4 3 4,5 1,5 Versicolor 5,6 3 4,5 1,5 Versicolor 6,4 2,9 4,3 1,3 Versicolor 4,9 2,5 4,5 1,7 Virginica 6 2,9 4,5 1,5 Versicolor 6,3 2,3 4,4 1,3 Versicolor 6 3,4 4,5 1,6 Versicolor 6,1 3 4,6 1,4 Versicolor 6,6 3 4,4 1,4 Versicolor 6,1 2,8 4,7 1,2 Versicolor 6,4 3,2 4,5 1,5 Versicolor 6,7 3,1 4,4 1,4 Versicolor 6,1 2,9 4,7 1,4 Versicolor 6,2 2,2 4,5 1,5 Versicolor 6,6 2,9 4,6 1,3 Versicolor 6,5 2,8 4,6 1,5 Versicolor 6,3 3,3 4,7 1,6 Versicolor 5,9 3,2 4,8 1,8 Versicolor 6 3 4,8 1,8 Virginica 6,7 3,1 4,7 1,5 Versicolor 6,2 2,8 4,8 1,8 Virginica 5,6 2,8 4,9 2 Virginica 6,3 2,5 4,9 1,5 Versicolor 6,1 3 4,9 1,8 Virginica 6 2,2 5 1,5 Virginica 6,8 2,8 4,8 1,4 Versicolor 7 3,2 4,7 1,4 Versicolor 6,3 2,7 4,9 1,8 Virginica 6 2,7 5,1 1,6 Versicolor 5,7 2,5 5 2 Virginica 5,9 3 5,1 1,8 Virginica 6,3 2,8 5,1 1,5 Virginica 5,8 2,7 5,1 1,9 Virginica 5,8 2,7 5,1 1,9 Virginica 6,9 3,1 4,9 1,5 Versicolor 6,3 2,5 5 1,9 Virginica 6,7 3 5 1,7 Versicolor 5,8 2,8 5,1 2,4 Virginica 6,5 3,2 5,1 2 Virginica 6,5 3 5,2 2 Virginica 6,4 2,7 5,3 1,9 Virginica 6,1 2,6 5,6 1,4 Virginica 6,4 3,1 5,5 1,8 Virginica 6,4 3,2 5,3 2,3 Virginica 6,5 3 5,5 1,8 Virginica 6,7 3 5,2 2,3 Virginica 6,3 2,9 5,6 1,8 Virginica 6,9 3,1 5,1 2,3 Virginica 6,2 3,4 5,4 2,3 Virginica 6,4 2,8 5,6 2,1 Virginica 6,9 3,1 5,4 2,1 Virginica 6,4 2,8 5,6 2,2 Virginica 6,8 3 5,5 2,1 Virginica 6,3 3,4 5,6 2,4 Virginica 6,7 3,3 5,7 2,1 Virginica 6,7 2,5 5,8 1,8 Virginica 6,7 3,1 5,6 2,4 Virginica 6,5 3 5,8 2,2 Virginica 7,2 3 5,8 1,6 Virginica 6,9 3,2 5,7 2,3 Virginica 6,7 3,3 5,7 2,5 Virginica 6,8 3,2 5,9 2,3 Virginica 6,3 3,3 6 2,5 Virginica 7,1 3 5,9 2,1 Virginica 7,2 3,2 6 1,8 Virginica 7,4 2,8 6,1 1,9 Virginica 7,3 2,9 6,3 1,8 Virginica 7,2 3,6 6,1 2,5 Virginica 7,7 3 6,1 2,3 Virginica 7,9 3,8 6,4 2 Virginica 7,6 3 6,6 2,1 Virginica 7,7 2,8 6,7 2 Virginica 7,7 3,8 6,7 2,2 Virginica 7,7 2,6 6,9 2,3 Virginica 1. Menghitung Jarak (a-sd1)^2 (b-sd2)^2 (c-sd3)^2 (d-sd4)^2 SQRT sqrt sorting 0 0,49 3,24 0,81 2,130728 0,141421356 0 0,36 3,61 0,64 2,147091 0,387298335 0,01 0,49 2,89 0,64 2,007486 0,458257569 0,01 0,36 2,89 0,64 1,974842 0,721110255 0,01 0,36 3,61 0,81 2,188607 0,787400787 0 0,49 3,24 0,64 2,090454 0,836660027 0,01 0,36 3,61 0,49 2,114237 0,848528137 0,01 0,81 3,61 0,64 2,251666 0,932737905 0,04 0,64 2,89 0,64 2,051828 0,959166305 0,04 0,64 4 0,64 2,306513 0,974679434 0,09 0,49 3,24 0,64 2,111871 0,974679434 0,01 1 3,24 0,64 2,211334 1,140175425 0,01 0,64 4,41 0,64 2,387467 1,174734012 0,01 1 2,89 0,64 2,130728 1,216552506 0,09 0,64 3,61 0,64 2,231591 1,236931688 0,04 1 3,24 0,64 2,218107 1,240967365 0,01 1 2,89 0,36 2,063977 1,256980509 0,09 1 3,61 0,64 2,310844 1,284523258 0,09 1 3,61 0,49 2,278157 1,319090596 0,04 0,81 2,56 0,25 1,913113 1,337908816 0,04 1,21 3,61 0,64 2,345208 1,360147051 0,04 1,21 3,61 0,49 2,313007 1,392838828 0,01 1,21 4 0,49 2,389561 1,476482306 0,09 1,21 3,24 0,64 2,275961 1,516575089 0,01 1 1,96 0,64 1,9 1,526433752 0,01 1,44 3,61 0,64 2,387467 1,555634919 0 1,44 3,61 0,81 2,420744 1,593737745 0,25 0,36 4 0,64 2,291288 1,740689519 0,25 0,64 4 0,64 2,351595 1,774823935 0,25 0,25 3,61 0,64 2,179449 1,808314132 0,01 1,21 2,89 0,16 2,066398 1,849324201 0,25 1 2,56 0,64 2,109502 1,894729532 0,25 1 3,24 0,36 2,202272 1,897366596 0,04 1,69 3,24 0,36 2,308679 1,9 0,16 1,69 3,24 0,64 2,393742 1,910497317 0,04 1,96 3,24 0,49 2,393742 1,913112647 0,04 1,96 2,89 0,64 2,351595 1,95192213 0,36 0,36 4,84 0,81 2,523886 1,974841766 0,36 1,21 4 0,64 2,491987 2,002498439 0,09 1,44 5,29 0,64 2,7313 2,00748599 0,25 1,69 3,24 0,64 2,412468 2,051828453 0,04 1,96 1,96 0,36 2,078461 2,051828453 0,16 0,01 4 0,49 2,158703 2,063976744 0,25 2,25 2,56 0,36 2,328089 2,066397832 0,25 2,25 4 0,36 2,61916 2,078460969 0,09 2,89 3,24 0,81 2,651415 2,090454496 0,64 1,96 2,56 0,49 2,376973 2,095232684 0,36 3,24 3,61 0,64 2,801785 2,109502311 0,81 2,56 4,41 0,64 2,901724 2,111871208 0,64 4 3,24 0,36 2,87054 2,111871208 0,04 0,01 0,09 0,01 0,387298 2,114237451 0,01 0,01 0 0 0,141421 2,128379665 0,64 0,04 0,04 0 0,848528 2,130727575 0,01 0,16 0,04 0 0,458258 2,130727575 0,49 0,25 0,09 0,09 0,959166 2,130727575 0,36 0 0,16 0 0,72111 2,140093456 0,36 0 0,25 0,01 0,787401 2,147091055 0,49 0,01 0,36 0,01 0,932738 2,149418526 0,09 0,09 0,36 0,16 0,83666 2,158703314 0,81 0,09 0,36 0,04 1,140175 2,158703314 0,36 0,01 0,49 0,09 0,974679 2,177154106 0,81 0,04 0,49 0,04 1,174734 2,179449472 0,81 0,09 0,64 0 1,240967 2,186321111 0,36 0,01 0,49 0,09 0,974679 2,188606863 0,49 0,36 0,64 0,09 1,256981 2,202271555 0,64 0,16 0,64 0,09 1,236932 2,211334439 1,21 0,04 0,49 0 1,319091 2,213594362 0,64 0,36 0,81 0,04 1,360147 2,218107301 1,44 0,16 0,49 0,09 1,476482 2,218107301 0,49 0,09 0,81 0,09 1,216553 2,224859546 0,64 0,25 0,81 0,09 1,337909 2,224859546 1 0,36 0,81 0,25 1,555635 2,23159136 0,36 0,04 1,21 0,04 1,284523 2,236067977 1,69 0,25 1 0,09 1,74069 2,25166605 0,64 0,16 1,44 0,09 1,526434 2,256102835 0,25 0,36 1,44 0,25 1,516575 2,258317958 0,49 0,36 1,44 0,25 1,593738 2,275961335 2,25 0,25 1 0,09 1,89473 2,27815715 0 0,01 1,44 0,49 1,392839 2,289104628 1,21 0,25 1,44 0,25 1,774824 2,291287847 1,96 0,01 1,21 0,09 1,808314 2,291287847 1,21 1 1,44 0,36 2,002498 2,306512519 1,44 0,36 1,69 0,16 1,910497 2,308679276 2,89 0,36 1,21 0,16 2,149419 2,310844002 1,44 0,16 1,96 0,04 1,897367 2,313006701 2,25 0,64 1,44 0,25 2,140093 2,328089345 3,24 0,49 1,21 0,16 2,258318 2,34520788 1,44 0,25 1,96 0,16 1,951922 2,351595203 1,69 0,04 1,44 0,25 1,849324 2,351595203 2,89 0,25 1,69 0,09 2,218107 2,368543856 2,56 0,16 1,69 0,25 2,158703 2,376972865 1,96 0,81 1,96 0,36 2,256103 2,38117618 1 0,64 2,25 0,64 2,12838 2,387467277 1,21 0,36 2,25 0,64 2,111871 2,387467277 3,24 0,49 1,96 0,25 2,437212 2,389560629 1,69 0,16 2,25 0,64 2,177154 2,393741841 0,49 0,16 2,56 1 2,051828 2,393741841 1,96 0,01 2,56 0,25 2,186321 2,412467616 1,44 0,36 2,56 0,64 2,236068 2,420743687 1,21 0,04 2,89 0,25 2,095233 2,437211521 3,61 0,16 2,25 0,16 2,485961 2,48394847 4,41 0,64 1,96 0,16 2,677686 2,485960579 1,96 0,09 2,56 0,64 2,291288 2,491987159 1,21 0,09 3,24 0,36 2,213594 2,523885893 0,64 0,01 2,89 1 2,130728 2,619160171 1 0,36 3,24 0,64 2,289105 2,632489316 1,96 0,16 3,24 0,25 2,368544 2,641968963 0,81 0,09 3,24 0,81 2,22486 2,651414717 0,81 0,09 3,24 0,81 2,22486 2,673948391 4 0,49 2,56 0,25 2,701851 2,677685568 1,96 0,01 2,89 0,81 2,381176 2,701851217 3,24 0,36 2,89 0,49 2,641969 2,727636339 0,81 0,16 3,24 1,96 2,483948 2,731300057 2,56 0,64 3,24 1 2,727636 2,744084547 2,56 0,36 3,61 1 2,744085 2,801785145 2,25 0,09 4 0,81 2,673948 2,853068524 1,44 0,04 5,29 0,16 2,632489 2,867054237 2,25 0,49 4,84 0,64 2,867054 2,870540019 2,25 0,64 4 1,69 2,929164 2,898275349 2,56 0,36 4,84 0,64 2,898275 2,901723626 3,24 0,36 3,61 1,69 2,983287 2,929163703 1,96 0,25 5,29 0,64 2,853069 2,964793416 4 0,49 3,24 1,69 3,069202 2,983286778 1,69 1 4,41 1,69 2,964793 2,984962311 2,25 0,16 5,29 1,21 2,984962 3,023243292 4 0,49 4,41 1,21 3,179623 3,069201851 2,25 0,16 5,29 1,44 3,023243 3,165438358 3,61 0,36 4,84 1,21 3,165438 3,179622619 1,96 1 5,29 1,96 3,195309 3,184336666 3,24 0,81 5,76 1,21 3,319639 3,195309062 3,24 0,01 6,25 0,64 3,184337 3,257299495 3,24 0,49 5,29 1,96 3,313608 3,313608305 2,56 0,36 6,25 1,44 3,257299 3,319638535 5,29 0,36 6,25 0,36 3,501428 3,472751071 4 0,64 5,76 1,69 3,477068 3,47706773 3,24 0,81 5,76 2,25 3,472751 3,50142828 3,61 0,64 6,76 1,69 3,563706 3,508560959 1,96 0,81 7,29 2,25 3,508561 3,563705936 4,84 0,36 6,76 1,21 3,629049 3,629049462 5,29 0,64 7,29 0,64 3,722902 3,722902094 6,25 0,16 7,84 0,81 3,880722 3,880721582 5,76 0,25 9 0,64 3,956008 3,956008089 5,29 1,44 7,84 2,25 4,101219 4,101219331 7,84 0,36 7,84 1,69 4,210701 4,210700654 9 1,96 9,61 1 4,644351 4,444097209 7,29 0,36 10,89 1,21 4,444097 4,53431362 7,84 0,16 11,56 1 4,534314 4,644351408 7,84 1,96 11,56 1,44 4,774935 4,746577715 7,84 0,04 12,96 1,69 4,746578 4,774934555 Class : Versicolor K=5 e 1/e setosa versicolor virginica 0,141421 7,07106782 0 7,07106782 0 0,387298 2,58198889 0 2,58198889 0 0,458258 2,1821789 0 2,1821789 0 0,72111 1,38675049 0 1,38675049 0 0,787401 1,27000127 0 1,27000127 0 FUZZY C MEANS \u200b K-Means Clustering adalah salah satu algoritma klasifikasi data yang cukup banyak dipakai untuk memecahkan masalah. Hanya saja metode tersebut tidak memiliki nilai pengembalian berupa sebuah nilai pembanding untuk masing-masing cluster, sehingga digunakan algoritma Fuzzy untuk menghitung skor dari sebuah data. Berikut untuk cara perhitungan data dengan fuzzy c means: Input data yang akan dicluster X , berupa matriks berukuran n x m ( n =jumlah sample data, m =atribut setiap data). Xij =data sample ke- i ( i =1,2,\u2026, n ), atribut ke- j ( j =1,2,\u2026, m ). Tentukan : Jumlah cluster = c Pangkat = w Maksimum iterasi = MaxIter Error terkecil yang diharapkan = \u03be Fungsi obyektif awal = Po =0 Iterasi awal = t = Bangkitkan nilai acak \u03bcik, i=1,2,\u2026,n; k=1,2,\u2026,c sebagai elemen-elemen matriks partisi awal \u03bcik. \u03bcik adalah derajat keanggotaan yang merujuk pada seberapa besar kemungkinan suatu data bisa menjadi anggota ke dalam suatu cluster.Posisi dan nilai matriks dibangun secara random. Dimana nilai keangotaan terletak pada interval 0 sampai dengan 1. Pada posisi awal matriks partisi U masih belum akurat begitu juga pusat clusternya. Sehingga kecendrungan data untuk masuk suatu cluster juga belum akurat. Hitung pusat Cluster ke-k: Vkj ,dengan k =1,2,\u2026c dan j =1,2,\u2026m. dimana Xij adalah variabel fuzzy yang digunakan dan w adalah bobot. Hitung fungsi obyektif pada iterasi ke- t , Pt Perhitungan fungsi objektif Pt dimana nilai variabel fuzzy Xij di kurang dengan dengan pusat cluster Vkj kemudian hasil pengurangannya di kuadradkan lalu masing-masing hasil kuadrad di jumlahkan untuk dikali dengan kuadrad dari derajat keanggotaan \u03bc ik untuk tiap cluster . Setelah itu jumlahkan semua nilai di semua cluster untuk mendapatkan fungsi objektif Pt. Hitung perubahan matriks partisi, dengan: i =1,2,\u2026n dan k =1,2,.. c . Untuk mencari perubahan matrik partisi \u03bc ik,pengurangan nilai variabel fuzzy Xij di lakukan kembali terhadap pusat cluster Vkj lalu dikuadradkan. Kemudian dijumlahkan lalu dipangkatkan dengan -1/( w -1) dengan bobot, w =2 hasilnya setiap data dipangkatkan dengan -1. Setelah proses perhitungan dilakukan, normalisasikan semua data derajat keanggotaan baru dengan cara menjumlahkan derajat keanggotaan baru k =1,\u2026 c , hasilnya kemudian dibagi dengan derajat keanggotaan yang baru. Proses ini dilakukan agar derajat keanggotaan yang baru mempunyai rentang antara 0 dan tidak lebih dari 1. Cek kondisi berhenti:a) jika:( |Pt \u2013 Pt-1 |< \u03be ) atau ( t >maxIter) maka berhenti.b) jika tidak, t=t +1, ulangi langkah ke-4. codingnya: import pandas as pd from pandas import DataFrame import random import numpy as np from IPython.display import HTML, display from tabulate import tabulate from math import log from sklearn.feature_selection import mutual_info_classif def table(df): display(HTML(tabulate(df, tablefmt='html', headers='keys', showindex=False))) Data = read_csv('leaf.csv', sep=',') Data = Data[['Eccentricity','Solidity', 'Lobedness', 'Entropy']].sample(6, random_state=42) D = Data.values print(\"Table (D) >>\") table(D) n, m, c, w, T, e, P0, t = *D.shape, 3, 2, 10, 0.1, 0, 1 print(\"Variables >>\") print(\" n = %d\\n m = %d\\n c = %d\\n w = %d\\n T = %d\\n e = %f\\n P0 = %d\\n t = %d\" % (n, m, c, w, T, e, P0, t)) Variables >> n = 6 m = 5 c = 3 w = 2 T = 10 e = 0.100000 P0 = 0 t = 1 random.seed(42) U = np.array([[random.uniform(0, 1) for _ in range(c)] for _ in range(n)]) print(\"U >>\\n\") print(U) # Caution: NP Array is math-agnostic (column-by-column) def cluster(U, D, x, y): return sum([U[i,y]**w*D[i,x] for i in range(n)])/sum([U[i,y]**w for i in range(n)]) V = np.array([[cluster(U,D,x,y) for x in range(m)] for y in range(c)]) print(\"V >>\\n\") print(V) #perhitungan ulang matriks derajat klaster def converge(V,D,i,k): return (sum([(D[i,j]-V[k,j])**2 for j in range(m)])**(-1/(w-1)))/sum([sum([(D[i,j]-V[k,j])**2 for j in range(m)])**(-1/(w-1)) for k in range(c)]) print(\"U >>\\n\") np.array([[converge(V,D,i,k) for k in range(c)] for i in range(n)]) #perhitungan literasi pemberhentian def iterate(U): V = np.array([[cluster(U, D, x, y) for x in range(m)] for y in range(c)]) return np.array([[converge(V,D,i,k) for k in range(c)] for i in range(n)]), objective(V,U,D) def fuzzyCM(U): #U = np.array([[random.uniform(0, 1) for _ in range(c)] for _ in range(n)]) U, P2, P, t = *iterate(U), 0, 1 while abs(P2 - P) > e and t < T: U, P2, P, t = *iterate(U), P2, t+1 return U, t FuzzyResult, FuzzyIters = fuzzyCM(U) print(\"Iterating %d times, fuzzy result >> \\n\" % FuzzyIters) print(FuzzyResult) #pengambilan rata-rata/nilai besar pada klaster table(DataFrame([D[i].tolist()+[np.argmax(FuzzyResult[i].tolist())] for i in range(n)], columns=Data.columns.tolist()+[\"Cluster Index\"])) 6 0 75 200 16 6 1 63 200 10 6 0 66 50 1 6 1 57 200 9 6 0 81 200 18 6 0 67 200 13 6 0 75 200 16 2 6 1 63 200 10 1 6 0 66 50 1 0 6 1 57 200 9 1 6 0 81 200 18 2 6 0 67 200 13 1","title":"Home"},{"location":"#nama-oktaviana-dwi-sujatmiko","text":"","title":"Nama : Oktaviana Dwi Sujatmiko"},{"location":"#nim-180411100021","text":"","title":"NIM : 180411100021"},{"location":"#teknik-informatika","text":"","title":"TEKNIK INFORMATIKA"},{"location":"#penambangan-data-3b","text":"","title":"PENAMBANGAN DATA 3B"},{"location":"#statistik-deskriptif","text":"Statistik deskriptif adalah salah satu bagian dari ilmu statistika yang berhubungan dengan aktivitas penghimpunan, penataan, peringkasan dan penyajian data dengan harapan agar data lebih bermakna, mudah dibaca dan mudah dipahami oleh pengguna data. Statistik deskriptif hanya sebatas memberikan deskripsi atau gambaran umum tentang karakteristik objek yang diteliti tanpa maksud untuk melakukan generalisasi sampel terhadap populasi. \u200b Kegiatan dalam statistik deskriptif meliputi pengumpulan, pengelompokan dan pengolahan data yang selanjutnya akan menghasilkan ukuran-ukuran statistik seperti frekuensi, pemusatan data, penyebaran data, kecenderungan suatu gugus data dan lain-lain. Selain itu, agar data lebih mudah dibaca dan dipahami maka data dapat diringkas dalam bentuk tabulasi atau disajikan dalam bentuk grafik atau diagram, beberapa yang akan kita bahas yaitu : Rata-rata Median Modus Variansi Standar Deviasi Range Quartil Skewness ## 1. Rata - rata \u200b Rata-rata adalah nilai yang mewakili sekelompok data. Rata-rata diperoleh dari penjumlahan seluruh nilai data dibagi dengan banyaknya data. ## 2. Median \u200b Untuk data miring (asimetris), ukuran pusat data yang lebih baik adalah median, yang merupakan nilai tengah dalam satu set nilai data yang diurutkan. Ini adalah nilai yang memisahkan separuh data yang lebih tinggi dari data tersebut dan sebagian data yang lebih rendah dari data tersebut. Dalam probabilitas dan statistik, median umumnya berlaku untuk data numerik; namun, kami dapat memperluas konsep menjadi data ordinal. Misalkan kumpulan N data yang diberikan untuk atribut X diurutkan dalam urutan naik. Jika N ganjil, maka median adalah nilai tengah dari data yang ordinal. Jika N adalah genap, maka mediannya tidak unik; dihitung dengan rata rata dari nilai","title":"Statistik Deskriptif"},{"location":"#3-modus","text":"Modus adalah nilai yang sering muncul dalam sekelompok data.","title":"3.  Modus"},{"location":"#4-variansi-dan-standart-variansi","text":"Varian adalah rata-rata jarak kuadrat antara data ke rata-rata.","title":"4. Variansi dan standart Variansi"},{"location":"#5-range","text":"Jangkauan (Range) adalah ukuran dari selisih nilai maksimum dengan nilai minimum yang ada dalam sekelompok data.R = x_{max} - x_{min} R =xmax\u2212xmin","title":"5. Range"},{"location":"#6-quartil","text":"Kuartil memberikan gambaran pusat distribus, penyebaran, dan bentuk distribusi. Kuartil satu, dilambangkan oleh Q1, adalah persentil ke-25. Nilai ini menunjukan 25% terendah dari data. Kuartil ketiga, dilambangkan oleh Q3, adalah persentil ke-75 - itu memisahkan data 75% dari terendah data (atau 25% dari tertinggi data.","title":"6. Quartil"},{"location":"#7-skewness","text":"\u200b Derajat distorsi dari kurva lonceng simetris atau distribusi normal. Ini mengukur kurangnya simetri dalam distribusi data Untuk menghitung derajat distorisi dapat menggunakan Koefisien Kemencengan Pearson yang diperoleh dengan menggunakan nilai selisih rata-rata dengan modus dibagi simpangan baku.","title":"7. Skewness"},{"location":"#mengukur-jarak-atribut-binary","text":"","title":"Mengukur Jarak Atribut Binary"},{"location":"#mengukur-jarak-tipe-categorical","text":"","title":"Mengukur Jarak Tipe categorical"},{"location":"#overlay-metric","text":"Ketika semua atribut adalah bertipe nominal, ukuran jarak yang paling sederhana adalah dengan Ovelay Metric (OM) yang dinyatakan dengan dimana nn adalah banyaknya atribut, ai(x)ai(x) dan ai(y)ai(y) adalah nilai atribut ke ii yaitu AiAi dari masing masing objek xx dan yy, \u03b4 (ai(x),ai(y))\u03b4 (ai(x),ai(y)) adalah 0 jika ai(x)=ai(y)ai(x)=ai(y) dan 1 jika sebaliknya. OM banyak digunakan oleh instance-based learning dan locally weighted learning. Jelas sekali , ini sedikit beruk untuk mengukur jarak antara masing-masing pasangan sample, karena gagal memanfaatkan tambahan informasi yang diberikan oleh nilai atribut nominal yang bisa membantu dalam generalisasi.","title":"Overlay Metric"},{"location":"#value-difference-metric-vdm","text":"VDM dikenalkan oleh Standfill and Waltz, versi sederhana dari VDM tanpa skema pembobotan didefinsisikan dengan dimana CCadalah banyaknya kelas, P(c|ai(x))P(c|ai(x)) adalah probabilitas bersyarat dimana kelas xx adalah cc dari atribut AiAi, yang memiliki nilai ai(x)ai(x), P(c|ai(y))P(c|ai(y)) adalah probabilitas bersyarat dimana kelas yy adalah cc dengan atribut AiAi memiliki nilai ai(y)ai(y) VDM mengasumsikan bahwa dua nilai dari atribut adalah lebih dekat jika memiliki klasifikasi sama. Pendekatan lain berbasi probabilitas adalah SFM (Short and Fukunaga Metric) yang kemudian dikembangkan oleh Myles dan Hand dan didefinisikan.","title":"Value Difference Metric (VDM)"},{"location":"#minimum-risk-metric-mrm","text":"Ukuran ini dipresentasikan oleh Blanzieri and Ricci, berbeda dari SFM yaitu meminimumkan selisih antara kesalahan berhingga dan kesalahan asymtotic. MRM meminimumkan risk of misclassification yang didefinisikan.","title":"Minimum Risk Metric (MRM)"},{"location":"#mengukur-jarak-tipe-ordinal","text":"Nilai-nilai atribut ordinal memiliki urutan atau peringkat, namun besarnya antara nilai-nilai berturut-turut tidak diketahui. Contohnya tingkatan kecil, sedang, besar untuk atribut ukuran. Atribut ordinal juga dapat diperoleh dari diskritisasi atribut numerik dengan membuat rentang nilai ke dalam sejumlah kategori tertentu. Kategori-kategori ini disusun dalam peringkat. Yaitu, rentang atribut numerik dapat dipetakan ke atribut ordinal ff yang memiliki MfMf state. Misalnya, kisaran suhu atribut skala-skala (dalam Celcius)dapat diatur ke dalam status berikut: \u221230 hingga \u221210, \u221210 hingga 10, 10 hingga 30, masing-masing mewakili kategori suhu dingin, suhu sedang, dan suhu hangat. MM adalah jumlah keadaan yang dapat dilakukan oleh atribut ordinalmemiliki. State ini menentukan peringkat 1,...,Mf1,...,Mf Perlakuan untuk atribut ordinal adalah cukup sama dengan atribut numerik ketika menghitung disimilarity antara objek. Misalkan ff adalah atribut-atribut dari atribut ordinal dari nn objek. Menghitung disimilarity terhadap f fitur sebagai berikut: Nilai ff untuk objek ke-ii adalah xifxif, dan ff memiliki MfMf status urutan , mewakili peringkat 1,..,Mf1,..,Mf Ganti setiap xifxif dengan peringkatnya, rif\u2208{1...Mf}rif\u2208{1...Mf} Karena setiap atribut ordinal dapat memiliki jumlah state yang berbeda, diperlukan untuk memetakan rentang setiap atribut ke [0,0, 1.0] sehingga setiap atribut memiliki bobot yang sama. Perl melakukan normalisasi data dengan mengganti peringkat rifrif denganzif=rif\u22121Mf\u22121zif=rif\u22121Mf\u22121 Dissimilarity kemudian dihitung dengan menggunakan ukuran jarak seperti atribut numerik dengan data yang baru setelah ditransformasi $ z _ { i f } ## Menghitung Jarak Tipe Campuran Menghitung ketidaksamaan antara objek dengan atribut campuran yang berupa nominal, biner simetris, biner asimetris, numerik, atau ordinal yang ada pada kebanyakan databasae dapat dinyatakan dengan memproses semua tipe atribut secara bersamaan. Salah satu teknik tersebut menggabungkan atribut yang berbeda ke dalam matriks ketidaksamaan tunggal dan menyatakannya dengan skala interval antar [0,0,1.0][0,0,1.0]. Misalkan data berisi atribut pp tipe campuran. Ketidaksamaan (disimilarity ) antara objek ii dan jj dinyatakan dengan d(i,j)=\u2211pf=1\u03b4(f)ijd(f)ij\u2211pf=1\u03b4(f)ijd(i,j)=\u2211f=1p\u03b4ij(f)dij(f)\u2211f=1p\u03b4ij(f) dimana \u03b4fij=0\u03b4ijf=0 - jika xifxif atau xjfxjf adalah hilang (i.e., tidak ada pengukuran dari atribut f untuk objek ii atau objek jj) jika xif=xjf=0xif=xjf=0 dan atribut ff adalah binary asymmetric, selain itu \u03b4fij=1\u03b4ijf=1 Kontribusi dari atribut ff untuk dissimilarity antara i dan j (yaitu.dfijdijf) dihitung bergantung pada tipenya, Jika ff adalah numerik, dfij=\u2225xif\u2212xjf\u2225maxhxhf\u2212minhxhfdijf=\u2016xif\u2212xjf\u2016maxhxhf\u2212minhxhf, di mana h menjalankan semua nilai objek yang tidak hilang untuk atribut f Jika ff adalah nominal atau binary,$d_{ij}^{f}=0 $jika xif=xjfxif=xjf, sebaliknya dfij=1dijf=1 Jika ff adalah ordinal maka hitung rangking rifrif dan zif=rif\u22121Mf\u22121zif=rif\u22121Mf\u22121 , dan perlakukan zifzif sebagai numerik.","title":"Mengukur Jarak Tipe Ordinal"},{"location":"#selection-feature","text":"Pemilihan (seleksi) data dari sekumpulan data operasional perlu dilakukan sebelum tahap penggalian informasi dalam KDD dimulai. Data hasil seleksi yang digunakan untuk proses data mining, disimpan dalam suatu berkas, terpisah dari basis data operasional. Entropy Target Entropy (S) merupakan jumlah bit yang diperkirakan dibutuhkan untuk dapat mengekstrak suatu kelas (+ atau -) dari sejumlah data acak pada ruang sampel S. Entropy dapat dikatakan sebagai kebutuhan bit untuk menyatakan suatu kelas. Entropy digunakan untuk mengukur ketidakaslian S. Information GAIN Gain (S,A) merupakan perolehan informasi dari atribut A relative terhadap output data S. Perolehan informasi didapat dari output data atau variable dependent S yang dikelompokkan berdasarkan atribut A, dinotasikan dengan gain (S,A).","title":"SELECTION FEATURE"},{"location":"#naive-bayes","text":"Algoritma Naive Bayes merupakan sebuah metoda klasifikasi menggunakan metode probabilitas dan statistik yg dikemukakan oleh ilmuwan Inggris Thomas Bayes. Algoritma Naive Bayes memprediksi peluang di masa depan berdasarkan pengalaman di masa sebelumnya sehingga dikenal sebagai Teorema Bayes. Ciri utama dr Na\u00efve Bayes Classifier ini adalah asumsi yg sangat kuat (na\u00eff) akan independensi dari masing-masing kondisi / kejadian. Naive Bayes Classifier bekerja sangat baik dibanding dengan model classifier lainnya. Hal ini dibuktikan pada jurnal Xhemali, Daniela, Chris J. Hinde, and Roger G. Stone. \u201cNaive Bayes vs. decision trees vs. neural networks in the classification of training web pages.\u201d (2009), mengatakan bahwa \u201cNa\u00efve Bayes Classifier memiliki tingkat akurasi yg lebih baik dibanding model classifier lainnya\u201d. Keuntungan penggunan adalah bahwa metoda ini hanya membutuhkan jumlah data pelatihan (training data) yang kecil untuk menentukan estimasi parameter yg diperlukan dalam proses pengklasifikasian. Karena yg diasumsikan sebagai variabel independent, maka hanya varians dari suatu variabel dalam sebuah kelas yang dibutuhkan untuk menentukan klasifikasi, bukan keseluruhan dari matriks kovarians. Tahapan dari proses algoritma Naive Bayes adalah: Menghitung jumlah kelas / label. Menghitung Jumlah Kasus Per Kelas Kalikan Semua Variable Kelas Bandingkan Hasil Per Kelas","title":"Naive Bayes"},{"location":"#1-kelebihan-kekurangan-naive-bayes","text":"Kelebihan Mudah untuk dibuat Hasil bagus Kekurangan Asumsi independence antar atribut membuat akurasi berkurang (karena biasanya ada keterkaitan) Keterangan : x : Data dengan class yang belum diketahui c : Hipotesis data merupakan suatu class spesifik P(c|x) : Probabilitas hipotesis berdasar kondisi (posteriori probability) P(c) : Probabilitas hipotesis (prior probability) P(x|c) : Probabilitas berdasarkan kondisi pada hipotesis P(x) : Probabilitas c Rumus diatas menjelaskan bahwa peluang masuknya sampel karakteristik tertentu dalam kelas C (Posterior) adalah peluang munculnya kelas C (sebelum masuknya sampel tersebut, seringkali disebut prior), dikali dengan peluang kemunculan karakteristik karakteristik sampel pada kelas C (disebut juga likelihood), dibagi dengan peluang kemunculan karakteristik sampel secara global ( disebut juga evidence). Nilai Evidence selalu tetap untuk setiap kelas pada satu sampel. Nilai dari posterior tersebut nantinya akan dibandingkan dengan nilai nilai posterior kelas lainnya untuk menentukan ke kelas apa suatu sampel akan diklasifikasikan. Penjabaran lebih lanjut rumus Bayes tersebut dilakukan dengan menjabarkan (c|x1,\u2026,xn) menggunakan aturan perkalian Dapat dilihat bahwa hasil penjabaran tersebut menyebabkan semakin banyak dan semakin kompleksnya faktor faktor syarat yang mempengaruhi nilai probabilitas, yang hampir mustahil untuk dianalisa satu persatu. Akibatnya, perhitungan tersebut menjadi sulit untuk dilakukan. Disinilah digunakan asumsi independensi yang sangat tinggi (naif), bahwa masing masing petunjuk saling bebas (independen) satu sama lain. Dengan asumsi tersebut, maka berlaku suatu kesamaan sebagai berikut: Persamaan diatas merupakan model dari Teorema Naive Bayes yang selanjutnya akan digunakan dalam proses klasifikasi. Untuk klasifikasi dengan data kontinyu digunakan rumus Densitas Gauss :","title":"1. Kelebihan &amp; Kekurangan Naive Bayes"},{"location":"#pengambilan-data-set","text":"from sklearn import datasets` `from pandas import *` `from numpy import *` `from math import *` `from IPython.display import HTML, display; from tabulate import tabulate` def table(df): display(HTML(tabulate(df, tablefmt='html', headers='keys', showindex=False)))` `from sklearn import datasets` `from pandas import *` `from numpy import *` `from math import *` `from IPython.display import HTML, display; from tabulate import tabulate` `def table(df): display(HTML(tabulate(df, tablefmt='html', headers='keys', showindex=False))) sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 4.4 3 1.3 0.2 setosa 5 3.6 1.4 0.2 setosa 7.7 3.8 6.7 2.2 virginica 7.2 3.2 6 1.8 virginica 5 3.4 1.6 0.4 setosa 6.4 2.7 5.3 1.9 virginica 5 3.5 1.3 0.3 setosa 6.2 2.2 4.5 1.5 versicolor 6.3 2.8 5.1 1.5 virginica 5.7 4.4 1.5 0.4 setosa 4.6 3.6 1 0.2 setosa 7.4 2.8 6.1 1.9 virginica 5.5 2.4 3.8 1.1 versicolor 6 3.4 4.5 1.6 versicolor 4.9 3.1 1.5 0.2 setosa 4.5 2.3 1.3 0.3 setosa 7.3 2.9 6.3 1.8 virginica 6.5 3 5.5 1.8 virginica 5.5 2.4 3.7 1 versicolor 6.9 3.2 5.7 2.3 virginica 7.2 3 5.8 1.6 virginica 4.9 3 1.4 0.2 setosa 6.8 3.2 5.9 2.3 virginica 5.6 3 4.5 1.5 versicolor 5 2.3 3.3 1 versicolor 5.6 2.5 3.9 1.1 versicolor 6.3 2.9 5.6 1.8 virginica 6.4 3.2 5.3 2.3 virginica 4.9 2.4 3.3 1 versicolor 4.9 3.6 1.4 0.1 setosa test = [5,6,8,9] print(\"sampel data: \", test) sampel data: [5, 6, 8, 9]","title":"pengambilan data set"},{"location":"#identifikasi-per-grub-class-target-untuk-data-mining-serta-perhitungan-probabilitas-prior-dan-likehood","text":"dataset_classes = {}` `for key,group in dataset.groupby('class'):` `mu_s = [group[c].mean() for c in group.columns[:-1]]` `sigma_s = [group[c].std() for c in group.columns[:-1]]` `dataset_classes[key] = [group, mu_s, sigma_s]` `print(key, \"===>\")` `print('Mu_s =>', array(mu_s))` `print('Sigma_s =>', array(sigma_s))` `table(group)` `def numericalPriorProbability(v, mu, sigma):` `return (1.0/sqrt(2 * pi * (sigma ** 2))*exp(-((v-mu)**2)/(2*(sigma**2))))` `def categoricalProbability(sample,universe):` `return sample.shape[0]/universe.shape[0]` `Ps = ([[y]+[numericalPriorProbability(x, d[1][i], d[2][i]) for i,x in enumerate(test)]+` `[categoricalProbability(d[0],dataset)] for y,d in dataset_classes.items()])` `table(DataFrame(Ps, columns=[\"classes\"]+[\"P( %d | C )\" % d for d in test]+[\"P( C )\"]))` `Pss = ([[r[0], prod(r[1:])] for r in Ps])` `PDss = DataFrame(Pss, columns=['class', 'probability']).sort_values('probability')[::-1]` `table(PDss) setosa ===> Mu_s => [4.89 3.35 1.37 0.25] Sigma_s => [0.36040101 0.55025247 0.16363917 0.09718253] sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 4.4 3 1.3 0.2 setosa 5 3.6 1.4 0.2 setosa 5 3.4 1.6 0.4 setosa 5 3.5 1.3 0.3 setosa 5.7 4.4 1.5 0.4 setosa 4.6 3.6 1 0.2 setosa 4.9 3.1 1.5 0.2 setosa 4.5 2.3 1.3 0.3 setosa 4.9 3 1.4 0.2 setosa 4.9 3.6 1.4 0.1 setosa versicolor ===> Mu_s => [5.5375 2.575 3.9375 1.225 ] Sigma_s => [0.44057592 0.40970373 0.51252178 0.26049404] sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 6.2 2.2 4.5 1.5 versicolor 5.5 2.4 3.8 1.1 versicolor 6 3.4 4.5 1.6 versicolor 5.5 2.4 3.7 1 versicolor 5.6 3 4.5 1.5 versicolor 5 2.3 3.3 1 versicolor 5.6 2.5 3.9 1.1 versicolor 4.9 2.4 3.3 1 versicolor virginica ===> Mu_s => [6.86666667 3.05833333 5.775 1.93333333] Sigma_s => [0.48679533 0.29374799 0.45949577 0.27743413] sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 7.7 3.8 6.7 2.2 virginica 7.2 3.2 6 1.8 virginica 6.4 2.7 5.3 1.9 virginica 6.3 2.8 5.1 1.5 virginica 7.4 2.8 6.1 1.9 virginica 7.3 2.9 6.3 1.8 virginica 6.5 3 5.5 1.8 virginica 6.9 3.2 5.7 2.3 virginica 7.2 3 5.8 1.6 virginica 6.8 3.2 5.9 2.3 virginica 6.3 2.9 5.6 1.8 virginica 6.4 3.2 5.3 2.3 virginica classes P( 5 | C ) P( 6 | C ) P( 8 | C ) P( 9 | C ) P( C ) setosa 1.05656 6.66694e-06 0 0 0.333333 versicolor 0.43022 6.50426e-16 1.77008e-14 5.48538e-194 0.266667 virginica 0.000525523 2.27126e-22 7.03175e-06 1.87532e-141 0.4 class probability virginica 6.29591e-172 versicolor 7.2453e-224 setosa 0","title":"identifikasi per grub class target untuk data mining serta perhitungan Probabilitas Prior dan Likehood"},{"location":"#percobaan","text":"`def predict(sampel):` `priorLikehoods = ([[y]+[numericalPriorProbability(x, d[1][i], d[2][i]) for i,x in enumerate(sampel)]+` `[categoricalProbability(d[0],dataset)] for y,d in dataset_classes.items()])` `products = ([[r[0], prod(r[1:])] for r in priorLikehoods])` `result = DataFrame(products, columns=['class', 'probability']).sort_values('probability')[::-1]` `return result.values[0,0]` `dataset_test = DataFrame([list(d)+[predict(d[:4])] for d in data], columns=list(dataset.columns)+['predicted class (by predict())'])` `table(dataset_test)` sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class predicted class (by predict()) 5.1 3.5 1.4 0.2 setosa setosa 4.9 3 1.4 0.2 setosa setosa 4.7 3.2 1.3 0.2 setosa setosa 4.6 3.1 1.5 0.2 setosa setosa 5 3.6 1.4 0.2 setosa setosa 5.4 3.9 1.7 0.4 setosa setosa 4.6 3.4 1.4 0.3 setosa setosa 5 3.4 1.5 0.2 setosa setosa 4.4 2.9 1.4 0.2 setosa setosa 4.9 3.1 1.5 0.1 setosa setosa 5.4 3.7 1.5 0.2 setosa setosa 4.8 3.4 1.6 0.2 setosa setosa 4.8 3 1.4 0.1 setosa setosa 4.3 3 1.1 0.1 setosa setosa 5.8 4 1.2 0.2 setosa setosa 5.7 4.4 1.5 0.4 setosa setosa 5.4 3.9 1.3 0.4 setosa setosa 5.1 3.5 1.4 0.3 setosa setosa 5.7 3.8 1.7 0.3 setosa setosa 5.1 3.8 1.5 0.3 setosa setosa 5.4 3.4 1.7 0.2 setosa setosa 5.1 3.7 1.5 0.4 setosa setosa 4.6 3.6 1 0.2 setosa setosa 5.1 3.3 1.7 0.5 setosa setosa 4.8 3.4 1.9 0.2 setosa setosa 5 3 1.6 0.2 setosa setosa 5 3.4 1.6 0.4 setosa setosa 5.2 3.5 1.5 0.2 setosa setosa 5.2 3.4 1.4 0.2 setosa setosa 4.7 3.2 1.6 0.2 setosa setosa 4.8 3.1 1.6 0.2 setosa setosa 5.4 3.4 1.5 0.4 setosa setosa 5.2 4.1 1.5 0.1 setosa setosa 5.5 4.2 1.4 0.2 setosa setosa 4.9 3.1 1.5 0.2 setosa setosa 5 3.2 1.2 0.2 setosa setosa 5.5 3.5 1.3 0.2 setosa setosa 4.9 3.6 1.4 0.1 setosa setosa 4.4 3 1.3 0.2 setosa setosa 5.1 3.4 1.5 0.2 setosa setosa 5 3.5 1.3 0.3 setosa setosa 4.5 2.3 1.3 0.3 setosa setosa 4.4 3.2 1.3 0.2 setosa setosa 5 3.5 1.6 0.6 setosa setosa 5.1 3.8 1.9 0.4 setosa setosa 4.8 3 1.4 0.3 setosa setosa 5.1 3.8 1.6 0.2 setosa setosa 4.6 3.2 1.4 0.2 setosa setosa 5.3 3.7 1.5 0.2 setosa setosa 5 3.3 1.4 0.2 setosa setosa 7 3.2 4.7 1.4 versicolor virginica 6.4 3.2 4.5 1.5 versicolor versicolor 6.9 3.1 4.9 1.5 versicolor virginica 5.5 2.3 4 1.3 versicolor versicolor 6.5 2.8 4.6 1.5 versicolor versicolor 5.7 2.8 4.5 1.3 versicolor versicolor 6.3 3.3 4.7 1.6 versicolor virginica 4.9 2.4 3.3 1 versicolor versicolor 6.6 2.9 4.6 1.3 versicolor versicolor 5.2 2.7 3.9 1.4 versicolor versicolor 5 2 3.5 1 versicolor versicolor 5.9 3 4.2 1.5 versicolor versicolor 6 2.2 4 1 versicolor versicolor 6.1 2.9 4.7 1.4 versicolor versicolor 5.6 2.9 3.6 1.3 versicolor versicolor 6.7 3.1 4.4 1.4 versicolor versicolor 5.6 3 4.5 1.5 versicolor versicolor 5.8 2.7 4.1 1 versicolor versicolor 6.2 2.2 4.5 1.5 versicolor versicolor 5.6 2.5 3.9 1.1 versicolor versicolor 5.9 3.2 4.8 1.8 versicolor virginica 6.1 2.8 4 1.3 versicolor versicolor 6.3 2.5 4.9 1.5 versicolor versicolor 6.1 2.8 4.7 1.2 versicolor versicolor 6.4 2.9 4.3 1.3 versicolor versicolor 6.6 3 4.4 1.4 versicolor versicolor 6.8 2.8 4.8 1.4 versicolor virginica 6.7 3 5 1.7 versicolor virginica 6 2.9 4.5 1.5 versicolor versicolor 5.7 2.6 3.5 1 versicolor versicolor 5.5 2.4 3.8 1.1 versicolor versicolor 5.5 2.4 3.7 1 versicolor versicolor 5.8 2.7 3.9 1.2 versicolor versicolor 6 2.7 5.1 1.6 versicolor virginica 5.4 3 4.5 1.5 versicolor versicolor 6 3.4 4.5 1.6 versicolor versicolor 6.7 3.1 4.7 1.5 versicolor virginica 6.3 2.3 4.4 1.3 versicolor versicolor 5.6 3 4.1 1.3 versicolor versicolor 5.5 2.5 4 1.3 versicolor versicolor 5.5 2.6 4.4 1.2 versicolor versicolor 6.1 3 4.6 1.4 versicolor versicolor 5.8 2.6 4 1.2 versicolor versicolor 5 2.3 3.3 1 versicolor versicolor 5.6 2.7 4.2 1.3 versicolor versicolor 5.7 3 4.2 1.2 versicolor versicolor 5.7 2.9 4.2 1.3 versicolor versicolor 6.2 2.9 4.3 1.3 versicolor versicolor 5.1 2.5 3 1.1 versicolor versicolor 5.7 2.8 4.1 1.3 versicolor versicolor 6.3 3.3 6 2.5 virginica virginica 5.8 2.7 5.1 1.9 virginica virginica 7.1 3 5.9 2.1 virginica virginica 6.3 2.9 5.6 1.8 virginica virginica 6.5 3 5.8 2.2 virginica virginica 7.6 3 6.6 2.1 virginica virginica 4.9 2.5 4.5 1.7 virginica versicolor 7.3 2.9 6.3 1.8 virginica virginica 6.7 2.5 5.8 1.8 virginica virginica 7.2 3.6 6.1 2.5 virginica virginica 6.5 3.2 5.1 2 virginica virginica 6.4 2.7 5.3 1.9 virginica virginica 6.8 3 5.5 2.1 virginica virginica 5.7 2.5 5 2 virginica virginica 5.8 2.8 5.1 2.4 virginica virginica 6.4 3.2 5.3 2.3 virginica virginica 6.5 3 5.5 1.8 virginica virginica 7.7 3.8 6.7 2.2 virginica virginica 7.7 2.6 6.9 2.3 virginica virginica 6 2.2 5 1.5 virginica versicolor 6.9 3.2 5.7 2.3 virginica virginica 5.6 2.8 4.9 2 virginica virginica 7.7 2.8 6.7 2 virginica virginica 6.3 2.7 4.9 1.8 virginica virginica 6.7 3.3 5.7 2.1 virginica virginica 7.2 3.2 6 1.8 virginica virginica 6.2 2.8 4.8 1.8 virginica virginica 6.1 3 4.9 1.8 virginica virginica 6.4 2.8 5.6 2.1 virginica virginica 7.2 3 5.8 1.6 virginica virginica 7.4 2.8 6.1 1.9 virginica virginica 7.9 3.8 6.4 2 virginica virginica 6.4 2.8 5.6 2.2 virginica virginica 6.3 2.8 5.1 1.5 virginica virginica 6.1 2.6 5.6 1.4 virginica virginica 7.7 3 6.1 2.3 virginica virginica 6.3 3.4 5.6 2.4 virginica virginica 6.4 3.1 5.5 1.8 virginica virginica 6 3 4.8 1.8 virginica virginica 6.9 3.1 5.4 2.1 virginica virginica 6.7 3.1 5.6 2.4 virginica virginica 6.9 3.1 5.1 2.3 virginica virginica 5.8 2.7 5.1 1.9 virginica virginica 6.8 3.2 5.9 2.3 virginica virginica 6.7 3.3 5.7 2.5 virginica virginica 6.7 3 5.2 2.3 virginica virginica 6.3 2.5 5 1.9 virginica virginica 6.5 3 5.2 2 virginica virginica 6.2 3.4 5.4 2.3 virginica virginica 5.9 3 5.1 1.8 virginica virginica corrects = dataset_test.loc[dataset_test['class'] == dataset_test['predicted class (by predict())']].shape[0] print('Prediksi Training Bayes: %d of %d == %f %%' % (corrects, len(data), corrects / len(data) * 100)) Prediksi Training Bayes: 140 of 150 == 93.333333 %","title":"Percobaan"},{"location":"#knn","text":"","title":"KNN"},{"location":"#pengertian-knn","text":"K-nearest neighbors atau knn adalah algoritma yang berfungsi untuk melakukan klasifikasi suatu data berdasarkan data pembelajaran ( train data sets ), yang diambil dari k tetangga terdekatnya ( nearest neighbors ). Dengan k merupakan banyaknya tetangga terdekat.","title":"Pengertian KNN"},{"location":"#a-cara-kerja-algoritma-k-nearest-neighbors-knn","text":"K-nearest neighbors melakukan klasifikasi dengan proyeksi data pembelajaran pada ruang berdimensi banyak. Ruang ini dibagi menjadi bagian-bagian yang merepresentasikan kriteria data pembelajaran. Setiap data pembelajaran direpresentasikan menjadi titik-titik c pada ruang dimensi banyak. sepal length sepal width petal length petal width variety sample data 4,9 2,4 3,3 1 Versicolor Training Data a b c d class 4,9 3,1 1,5 0,1 Setosa 4,9 3 1,4 0,2 Setosa 4,8 3,1 1,6 0,2 Setosa 5 3 1,6 0,2 Setosa 4,8 3 1,4 0,1 Setosa 4,9 3,1 1,5 0,2 Setosa 4,8 3 1,4 0,3 Setosa 5 3,3 1,4 0,2 Setosa 4,7 3,2 1,6 0,2 Setosa 4,7 3,2 1,3 0,2 Setosa 4,6 3,1 1,5 0,2 Setosa 5 3,4 1,5 0,2 Setosa 5 3,2 1,2 0,2 Setosa 4,8 3,4 1,6 0,2 Setosa 4,6 3,2 1,4 0,2 Setosa 5,1 3,4 1,5 0,2 Setosa 5 3,4 1,6 0,4 Setosa 5,2 3,4 1,4 0,2 Setosa 4,6 3,4 1,4 0,3 Setosa 5,1 3,3 1,7 0,5 Setosa 5,1 3,5 1,4 0,2 Setosa 5,1 3,5 1,4 0,3 Setosa 5 3,5 1,3 0,3 Setosa 5,2 3,5 1,5 0,2 Setosa 4,8 3,4 1,9 0,2 Setosa 5 3,6 1,4 0,2 Setosa 4,9 3,6 1,4 0,1 Setosa 4,4 3 1,3 0,2 Setosa 4,4 3,2 1,3 0,2 Setosa 4,4 2,9 1,4 0,2 Setosa 5 3,5 1,6 0,6 Setosa 5,4 3,4 1,7 0,2 Setosa 5,4 3,4 1,5 0,4 Setosa 5,1 3,7 1,5 0,4 Setosa 5,3 3,7 1,5 0,2 Setosa 5,1 3,8 1,5 0,3 Setosa 5,1 3,8 1,6 0,2 Setosa 4,3 3 1,1 0,1 Setosa 5,5 3,5 1,3 0,2 Setosa 4,6 3,6 1 0,2 Setosa 5,4 3,7 1,5 0,2 Setosa 5,1 3,8 1,9 0,4 Setosa 4,5 2,3 1,3 0,3 Setosa 5,4 3,9 1,7 0,4 Setosa 5,4 3,9 1,3 0,4 Setosa 5,2 4,1 1,5 0,1 Setosa 5,7 3,8 1,7 0,3 Setosa 5,5 4,2 1,4 0,2 Setosa 5,8 4 1,2 0,2 Setosa 5,7 4,4 1,5 0,4 Setosa 5,1 2,5 3 1,1 Versicolor 5 2,3 3,3 1 Versicolor 5,7 2,6 3,5 1 Versicolor 5 2 3,5 1 Versicolor 5,6 2,9 3,6 1,3 Versicolor 5,5 2,4 3,7 1 Versicolor 5,5 2,4 3,8 1,1 Versicolor 5,6 2,5 3,9 1,1 Versicolor 5,2 2,7 3,9 1,4 Versicolor 5,8 2,7 3,9 1,2 Versicolor 5,5 2,5 4 1,3 Versicolor 5,8 2,6 4 1,2 Versicolor 5,8 2,7 4,1 1 Versicolor 5,5 2,3 4 1,3 Versicolor 5,6 3 4,1 1,3 Versicolor 5,7 2,8 4,1 1,3 Versicolor 6 2,2 4 1 Versicolor 5,7 3 4,2 1,2 Versicolor 6,1 2,8 4 1,3 Versicolor 5,6 2,7 4,2 1,3 Versicolor 5,7 2,9 4,2 1,3 Versicolor 5,9 3 4,2 1,5 Versicolor 5,5 2,6 4,4 1,2 Versicolor 6,2 2,9 4,3 1,3 Versicolor 5,7 2,8 4,5 1,3 Versicolor 5,4 3 4,5 1,5 Versicolor 5,6 3 4,5 1,5 Versicolor 6,4 2,9 4,3 1,3 Versicolor 4,9 2,5 4,5 1,7 Virginica 6 2,9 4,5 1,5 Versicolor 6,3 2,3 4,4 1,3 Versicolor 6 3,4 4,5 1,6 Versicolor 6,1 3 4,6 1,4 Versicolor 6,6 3 4,4 1,4 Versicolor 6,1 2,8 4,7 1,2 Versicolor 6,4 3,2 4,5 1,5 Versicolor 6,7 3,1 4,4 1,4 Versicolor 6,1 2,9 4,7 1,4 Versicolor 6,2 2,2 4,5 1,5 Versicolor 6,6 2,9 4,6 1,3 Versicolor 6,5 2,8 4,6 1,5 Versicolor 6,3 3,3 4,7 1,6 Versicolor 5,9 3,2 4,8 1,8 Versicolor 6 3 4,8 1,8 Virginica 6,7 3,1 4,7 1,5 Versicolor 6,2 2,8 4,8 1,8 Virginica 5,6 2,8 4,9 2 Virginica 6,3 2,5 4,9 1,5 Versicolor 6,1 3 4,9 1,8 Virginica 6 2,2 5 1,5 Virginica 6,8 2,8 4,8 1,4 Versicolor 7 3,2 4,7 1,4 Versicolor 6,3 2,7 4,9 1,8 Virginica 6 2,7 5,1 1,6 Versicolor 5,7 2,5 5 2 Virginica 5,9 3 5,1 1,8 Virginica 6,3 2,8 5,1 1,5 Virginica 5,8 2,7 5,1 1,9 Virginica 5,8 2,7 5,1 1,9 Virginica 6,9 3,1 4,9 1,5 Versicolor 6,3 2,5 5 1,9 Virginica 6,7 3 5 1,7 Versicolor 5,8 2,8 5,1 2,4 Virginica 6,5 3,2 5,1 2 Virginica 6,5 3 5,2 2 Virginica 6,4 2,7 5,3 1,9 Virginica 6,1 2,6 5,6 1,4 Virginica 6,4 3,1 5,5 1,8 Virginica 6,4 3,2 5,3 2,3 Virginica 6,5 3 5,5 1,8 Virginica 6,7 3 5,2 2,3 Virginica 6,3 2,9 5,6 1,8 Virginica 6,9 3,1 5,1 2,3 Virginica 6,2 3,4 5,4 2,3 Virginica 6,4 2,8 5,6 2,1 Virginica 6,9 3,1 5,4 2,1 Virginica 6,4 2,8 5,6 2,2 Virginica 6,8 3 5,5 2,1 Virginica 6,3 3,4 5,6 2,4 Virginica 6,7 3,3 5,7 2,1 Virginica 6,7 2,5 5,8 1,8 Virginica 6,7 3,1 5,6 2,4 Virginica 6,5 3 5,8 2,2 Virginica 7,2 3 5,8 1,6 Virginica 6,9 3,2 5,7 2,3 Virginica 6,7 3,3 5,7 2,5 Virginica 6,8 3,2 5,9 2,3 Virginica 6,3 3,3 6 2,5 Virginica 7,1 3 5,9 2,1 Virginica 7,2 3,2 6 1,8 Virginica 7,4 2,8 6,1 1,9 Virginica 7,3 2,9 6,3 1,8 Virginica 7,2 3,6 6,1 2,5 Virginica 7,7 3 6,1 2,3 Virginica 7,9 3,8 6,4 2 Virginica 7,6 3 6,6 2,1 Virginica 7,7 2,8 6,7 2 Virginica 7,7 3,8 6,7 2,2 Virginica 7,7 2,6 6,9 2,3 Virginica 1. Menghitung Jarak (a-sd1)^2 (b-sd2)^2 (c-sd3)^2 (d-sd4)^2 SQRT sqrt sorting 0 0,49 3,24 0,81 2,130728 0,141421356 0 0,36 3,61 0,64 2,147091 0,387298335 0,01 0,49 2,89 0,64 2,007486 0,458257569 0,01 0,36 2,89 0,64 1,974842 0,721110255 0,01 0,36 3,61 0,81 2,188607 0,787400787 0 0,49 3,24 0,64 2,090454 0,836660027 0,01 0,36 3,61 0,49 2,114237 0,848528137 0,01 0,81 3,61 0,64 2,251666 0,932737905 0,04 0,64 2,89 0,64 2,051828 0,959166305 0,04 0,64 4 0,64 2,306513 0,974679434 0,09 0,49 3,24 0,64 2,111871 0,974679434 0,01 1 3,24 0,64 2,211334 1,140175425 0,01 0,64 4,41 0,64 2,387467 1,174734012 0,01 1 2,89 0,64 2,130728 1,216552506 0,09 0,64 3,61 0,64 2,231591 1,236931688 0,04 1 3,24 0,64 2,218107 1,240967365 0,01 1 2,89 0,36 2,063977 1,256980509 0,09 1 3,61 0,64 2,310844 1,284523258 0,09 1 3,61 0,49 2,278157 1,319090596 0,04 0,81 2,56 0,25 1,913113 1,337908816 0,04 1,21 3,61 0,64 2,345208 1,360147051 0,04 1,21 3,61 0,49 2,313007 1,392838828 0,01 1,21 4 0,49 2,389561 1,476482306 0,09 1,21 3,24 0,64 2,275961 1,516575089 0,01 1 1,96 0,64 1,9 1,526433752 0,01 1,44 3,61 0,64 2,387467 1,555634919 0 1,44 3,61 0,81 2,420744 1,593737745 0,25 0,36 4 0,64 2,291288 1,740689519 0,25 0,64 4 0,64 2,351595 1,774823935 0,25 0,25 3,61 0,64 2,179449 1,808314132 0,01 1,21 2,89 0,16 2,066398 1,849324201 0,25 1 2,56 0,64 2,109502 1,894729532 0,25 1 3,24 0,36 2,202272 1,897366596 0,04 1,69 3,24 0,36 2,308679 1,9 0,16 1,69 3,24 0,64 2,393742 1,910497317 0,04 1,96 3,24 0,49 2,393742 1,913112647 0,04 1,96 2,89 0,64 2,351595 1,95192213 0,36 0,36 4,84 0,81 2,523886 1,974841766 0,36 1,21 4 0,64 2,491987 2,002498439 0,09 1,44 5,29 0,64 2,7313 2,00748599 0,25 1,69 3,24 0,64 2,412468 2,051828453 0,04 1,96 1,96 0,36 2,078461 2,051828453 0,16 0,01 4 0,49 2,158703 2,063976744 0,25 2,25 2,56 0,36 2,328089 2,066397832 0,25 2,25 4 0,36 2,61916 2,078460969 0,09 2,89 3,24 0,81 2,651415 2,090454496 0,64 1,96 2,56 0,49 2,376973 2,095232684 0,36 3,24 3,61 0,64 2,801785 2,109502311 0,81 2,56 4,41 0,64 2,901724 2,111871208 0,64 4 3,24 0,36 2,87054 2,111871208 0,04 0,01 0,09 0,01 0,387298 2,114237451 0,01 0,01 0 0 0,141421 2,128379665 0,64 0,04 0,04 0 0,848528 2,130727575 0,01 0,16 0,04 0 0,458258 2,130727575 0,49 0,25 0,09 0,09 0,959166 2,130727575 0,36 0 0,16 0 0,72111 2,140093456 0,36 0 0,25 0,01 0,787401 2,147091055 0,49 0,01 0,36 0,01 0,932738 2,149418526 0,09 0,09 0,36 0,16 0,83666 2,158703314 0,81 0,09 0,36 0,04 1,140175 2,158703314 0,36 0,01 0,49 0,09 0,974679 2,177154106 0,81 0,04 0,49 0,04 1,174734 2,179449472 0,81 0,09 0,64 0 1,240967 2,186321111 0,36 0,01 0,49 0,09 0,974679 2,188606863 0,49 0,36 0,64 0,09 1,256981 2,202271555 0,64 0,16 0,64 0,09 1,236932 2,211334439 1,21 0,04 0,49 0 1,319091 2,213594362 0,64 0,36 0,81 0,04 1,360147 2,218107301 1,44 0,16 0,49 0,09 1,476482 2,218107301 0,49 0,09 0,81 0,09 1,216553 2,224859546 0,64 0,25 0,81 0,09 1,337909 2,224859546 1 0,36 0,81 0,25 1,555635 2,23159136 0,36 0,04 1,21 0,04 1,284523 2,236067977 1,69 0,25 1 0,09 1,74069 2,25166605 0,64 0,16 1,44 0,09 1,526434 2,256102835 0,25 0,36 1,44 0,25 1,516575 2,258317958 0,49 0,36 1,44 0,25 1,593738 2,275961335 2,25 0,25 1 0,09 1,89473 2,27815715 0 0,01 1,44 0,49 1,392839 2,289104628 1,21 0,25 1,44 0,25 1,774824 2,291287847 1,96 0,01 1,21 0,09 1,808314 2,291287847 1,21 1 1,44 0,36 2,002498 2,306512519 1,44 0,36 1,69 0,16 1,910497 2,308679276 2,89 0,36 1,21 0,16 2,149419 2,310844002 1,44 0,16 1,96 0,04 1,897367 2,313006701 2,25 0,64 1,44 0,25 2,140093 2,328089345 3,24 0,49 1,21 0,16 2,258318 2,34520788 1,44 0,25 1,96 0,16 1,951922 2,351595203 1,69 0,04 1,44 0,25 1,849324 2,351595203 2,89 0,25 1,69 0,09 2,218107 2,368543856 2,56 0,16 1,69 0,25 2,158703 2,376972865 1,96 0,81 1,96 0,36 2,256103 2,38117618 1 0,64 2,25 0,64 2,12838 2,387467277 1,21 0,36 2,25 0,64 2,111871 2,387467277 3,24 0,49 1,96 0,25 2,437212 2,389560629 1,69 0,16 2,25 0,64 2,177154 2,393741841 0,49 0,16 2,56 1 2,051828 2,393741841 1,96 0,01 2,56 0,25 2,186321 2,412467616 1,44 0,36 2,56 0,64 2,236068 2,420743687 1,21 0,04 2,89 0,25 2,095233 2,437211521 3,61 0,16 2,25 0,16 2,485961 2,48394847 4,41 0,64 1,96 0,16 2,677686 2,485960579 1,96 0,09 2,56 0,64 2,291288 2,491987159 1,21 0,09 3,24 0,36 2,213594 2,523885893 0,64 0,01 2,89 1 2,130728 2,619160171 1 0,36 3,24 0,64 2,289105 2,632489316 1,96 0,16 3,24 0,25 2,368544 2,641968963 0,81 0,09 3,24 0,81 2,22486 2,651414717 0,81 0,09 3,24 0,81 2,22486 2,673948391 4 0,49 2,56 0,25 2,701851 2,677685568 1,96 0,01 2,89 0,81 2,381176 2,701851217 3,24 0,36 2,89 0,49 2,641969 2,727636339 0,81 0,16 3,24 1,96 2,483948 2,731300057 2,56 0,64 3,24 1 2,727636 2,744084547 2,56 0,36 3,61 1 2,744085 2,801785145 2,25 0,09 4 0,81 2,673948 2,853068524 1,44 0,04 5,29 0,16 2,632489 2,867054237 2,25 0,49 4,84 0,64 2,867054 2,870540019 2,25 0,64 4 1,69 2,929164 2,898275349 2,56 0,36 4,84 0,64 2,898275 2,901723626 3,24 0,36 3,61 1,69 2,983287 2,929163703 1,96 0,25 5,29 0,64 2,853069 2,964793416 4 0,49 3,24 1,69 3,069202 2,983286778 1,69 1 4,41 1,69 2,964793 2,984962311 2,25 0,16 5,29 1,21 2,984962 3,023243292 4 0,49 4,41 1,21 3,179623 3,069201851 2,25 0,16 5,29 1,44 3,023243 3,165438358 3,61 0,36 4,84 1,21 3,165438 3,179622619 1,96 1 5,29 1,96 3,195309 3,184336666 3,24 0,81 5,76 1,21 3,319639 3,195309062 3,24 0,01 6,25 0,64 3,184337 3,257299495 3,24 0,49 5,29 1,96 3,313608 3,313608305 2,56 0,36 6,25 1,44 3,257299 3,319638535 5,29 0,36 6,25 0,36 3,501428 3,472751071 4 0,64 5,76 1,69 3,477068 3,47706773 3,24 0,81 5,76 2,25 3,472751 3,50142828 3,61 0,64 6,76 1,69 3,563706 3,508560959 1,96 0,81 7,29 2,25 3,508561 3,563705936 4,84 0,36 6,76 1,21 3,629049 3,629049462 5,29 0,64 7,29 0,64 3,722902 3,722902094 6,25 0,16 7,84 0,81 3,880722 3,880721582 5,76 0,25 9 0,64 3,956008 3,956008089 5,29 1,44 7,84 2,25 4,101219 4,101219331 7,84 0,36 7,84 1,69 4,210701 4,210700654 9 1,96 9,61 1 4,644351 4,444097209 7,29 0,36 10,89 1,21 4,444097 4,53431362 7,84 0,16 11,56 1 4,534314 4,644351408 7,84 1,96 11,56 1,44 4,774935 4,746577715 7,84 0,04 12,96 1,69 4,746578 4,774934555 Class : Versicolor K=5 e 1/e setosa versicolor virginica 0,141421 7,07106782 0 7,07106782 0 0,387298 2,58198889 0 2,58198889 0 0,458258 2,1821789 0 2,1821789 0 0,72111 1,38675049 0 1,38675049 0 0,787401 1,27000127 0 1,27000127 0","title":"A. Cara Kerja Algoritma K-Nearest Neighbors (KNN)"},{"location":"#fuzzy-c-means","text":"\u200b K-Means Clustering adalah salah satu algoritma klasifikasi data yang cukup banyak dipakai untuk memecahkan masalah. Hanya saja metode tersebut tidak memiliki nilai pengembalian berupa sebuah nilai pembanding untuk masing-masing cluster, sehingga digunakan algoritma Fuzzy untuk menghitung skor dari sebuah data. Berikut untuk cara perhitungan data dengan fuzzy c means: Input data yang akan dicluster X , berupa matriks berukuran n x m ( n =jumlah sample data, m =atribut setiap data). Xij =data sample ke- i ( i =1,2,\u2026, n ), atribut ke- j ( j =1,2,\u2026, m ). Tentukan : Jumlah cluster = c Pangkat = w Maksimum iterasi = MaxIter Error terkecil yang diharapkan = \u03be Fungsi obyektif awal = Po =0 Iterasi awal = t = Bangkitkan nilai acak \u03bcik, i=1,2,\u2026,n; k=1,2,\u2026,c sebagai elemen-elemen matriks partisi awal \u03bcik. \u03bcik adalah derajat keanggotaan yang merujuk pada seberapa besar kemungkinan suatu data bisa menjadi anggota ke dalam suatu cluster.Posisi dan nilai matriks dibangun secara random. Dimana nilai keangotaan terletak pada interval 0 sampai dengan 1. Pada posisi awal matriks partisi U masih belum akurat begitu juga pusat clusternya. Sehingga kecendrungan data untuk masuk suatu cluster juga belum akurat. Hitung pusat Cluster ke-k: Vkj ,dengan k =1,2,\u2026c dan j =1,2,\u2026m. dimana Xij adalah variabel fuzzy yang digunakan dan w adalah bobot. Hitung fungsi obyektif pada iterasi ke- t , Pt Perhitungan fungsi objektif Pt dimana nilai variabel fuzzy Xij di kurang dengan dengan pusat cluster Vkj kemudian hasil pengurangannya di kuadradkan lalu masing-masing hasil kuadrad di jumlahkan untuk dikali dengan kuadrad dari derajat keanggotaan \u03bc ik untuk tiap cluster . Setelah itu jumlahkan semua nilai di semua cluster untuk mendapatkan fungsi objektif Pt. Hitung perubahan matriks partisi, dengan: i =1,2,\u2026n dan k =1,2,.. c . Untuk mencari perubahan matrik partisi \u03bc ik,pengurangan nilai variabel fuzzy Xij di lakukan kembali terhadap pusat cluster Vkj lalu dikuadradkan. Kemudian dijumlahkan lalu dipangkatkan dengan -1/( w -1) dengan bobot, w =2 hasilnya setiap data dipangkatkan dengan -1. Setelah proses perhitungan dilakukan, normalisasikan semua data derajat keanggotaan baru dengan cara menjumlahkan derajat keanggotaan baru k =1,\u2026 c , hasilnya kemudian dibagi dengan derajat keanggotaan yang baru. Proses ini dilakukan agar derajat keanggotaan yang baru mempunyai rentang antara 0 dan tidak lebih dari 1. Cek kondisi berhenti:a) jika:( |Pt \u2013 Pt-1 |< \u03be ) atau ( t >maxIter) maka berhenti.b) jika tidak, t=t +1, ulangi langkah ke-4. codingnya: import pandas as pd from pandas import DataFrame import random import numpy as np from IPython.display import HTML, display from tabulate import tabulate from math import log from sklearn.feature_selection import mutual_info_classif def table(df): display(HTML(tabulate(df, tablefmt='html', headers='keys', showindex=False))) Data = read_csv('leaf.csv', sep=',') Data = Data[['Eccentricity','Solidity', 'Lobedness', 'Entropy']].sample(6, random_state=42) D = Data.values print(\"Table (D) >>\") table(D) n, m, c, w, T, e, P0, t = *D.shape, 3, 2, 10, 0.1, 0, 1 print(\"Variables >>\") print(\" n = %d\\n m = %d\\n c = %d\\n w = %d\\n T = %d\\n e = %f\\n P0 = %d\\n t = %d\" % (n, m, c, w, T, e, P0, t)) Variables >> n = 6 m = 5 c = 3 w = 2 T = 10 e = 0.100000 P0 = 0 t = 1 random.seed(42) U = np.array([[random.uniform(0, 1) for _ in range(c)] for _ in range(n)]) print(\"U >>\\n\") print(U) # Caution: NP Array is math-agnostic (column-by-column) def cluster(U, D, x, y): return sum([U[i,y]**w*D[i,x] for i in range(n)])/sum([U[i,y]**w for i in range(n)]) V = np.array([[cluster(U,D,x,y) for x in range(m)] for y in range(c)]) print(\"V >>\\n\") print(V) #perhitungan ulang matriks derajat klaster def converge(V,D,i,k): return (sum([(D[i,j]-V[k,j])**2 for j in range(m)])**(-1/(w-1)))/sum([sum([(D[i,j]-V[k,j])**2 for j in range(m)])**(-1/(w-1)) for k in range(c)]) print(\"U >>\\n\") np.array([[converge(V,D,i,k) for k in range(c)] for i in range(n)]) #perhitungan literasi pemberhentian def iterate(U): V = np.array([[cluster(U, D, x, y) for x in range(m)] for y in range(c)]) return np.array([[converge(V,D,i,k) for k in range(c)] for i in range(n)]), objective(V,U,D) def fuzzyCM(U): #U = np.array([[random.uniform(0, 1) for _ in range(c)] for _ in range(n)]) U, P2, P, t = *iterate(U), 0, 1 while abs(P2 - P) > e and t < T: U, P2, P, t = *iterate(U), P2, t+1 return U, t FuzzyResult, FuzzyIters = fuzzyCM(U) print(\"Iterating %d times, fuzzy result >> \\n\" % FuzzyIters) print(FuzzyResult) #pengambilan rata-rata/nilai besar pada klaster table(DataFrame([D[i].tolist()+[np.argmax(FuzzyResult[i].tolist())] for i in range(n)], columns=Data.columns.tolist()+[\"Cluster Index\"])) 6 0 75 200 16 6 1 63 200 10 6 0 66 50 1 6 1 57 200 9 6 0 81 200 18 6 0 67 200 13 6 0 75 200 16 2 6 1 63 200 10 1 6 0 66 50 1 0 6 1 57 200 9 1 6 0 81 200 18 2 6 0 67 200 13 1","title":"FUZZY C MEANS"}]}